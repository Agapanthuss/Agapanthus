
一  机器学习基础
如何调参? 是怎么做的
1.1	评价指标—分类
混淆矩阵:




真实		预测
		1（Postive）	0（Negative）
	1（Postive）	TP 
(真正)	FN
(假负)
	0（Negative）	FP
(假正)	TN
(真负)

1.1.1准确度
 
虽然准确率适用范围很广，可用于多分类以及多标签等问题上，但在多标签问题上很严格，在有些情况下区分度较差。
1.1.2 精确率与召回率
精确率与召回率多用于二分类问题。
精确率（Precision）指的是模型判为正的所有样本中有多少是真正的正样本；
精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正为正的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)
 P = TP/(TP+FP)
 
召回率（Recall）指的是所有实际为正的正样本有多少被模型预测出来了，即召回。
而召回率是针对我们真实的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。
  R = TP/(TP+FN)          
 
有时候我们需要在精确率与召回率间进行权衡，一种选择是画出精确率-召回率曲线,曲线下的面积被称为AP分数（Average precision score）；另外一种选择是计算Fβ分数,
将准确率和召回率这两个分值合并为一个分值
1.1.3 Fβ分数
F1 Score为精准率和召回率的调和均值
 
当β=1称为F1分数，是自然语言处理领域 分类与信息检索中最常用的指标之一。 
当β=最低值 0时,为精度
当β=无穷大时, 为召回Recall
所以如果接近 0，则得出接近精度的值，如果很大，则得出接近召回率的值，如果 β=1, 则得出精度和召回率的调和平均数。
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('Accuracy score: ', format(accuracy_score(y_test, predictions)))
print('Precision score: ', format(precision_score(y_test, predictions)))
print('Recall score: ', format(recall_score(y_test, predictions)))
print('F1 score: ', format(f1_score(y_test, predictions)))
1.1.4 ROC 曲线
ROC曲线适用于二分类问题,是以真正率(灵敏度)为从坐标,假正率为横坐标绘制的曲线,
AUC分数是曲线下的面积,面积越大意味着分类器的效果越好.

真正率= TP/(TP+FN)  所有正的 真正以及假负
假正率= FP/(TN+FP)  所有负的 真负以及假正
AUC = 1：绝对完美分类器，理想状态下，100%完美识别正负类，不管阈值怎么设定都能得出完美预测，绝大多数预测不存在完美分类器； 
0.5<AUC<1：优于随机猜测。这个分类器（模型）妥善设定阈值的话，可能有预测价值； 
AUC=0.5：跟随机猜测一样（例：随机丢N次硬币，正反出现的概率为50%），模型没有预测价值； 
AUC<0.5：比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在AUC<0.5的状况。

1.1.5对数损失
对数损失（Log loss）亦被称为逻辑回归损失（Logistic regression loss）或交叉熵损失（Cross-entropy loss）。
对于二分类问题，设y∈{0,1} 且p=Pr(y=1) ，则对每个样本的对数损失为：
 
对于多分类问题,设Y为指示矩阵，即当样本i的分类为k时yi,k=1；设P为估计的概率矩阵，即pi,k=Pr(ti,k=1)，则对每个样本的对数损失为：
 
1.2	评价指标—回归
1.2.1平均绝对误差
拟合问题比较简单，所用到的衡量指标也相对直观。假设yi是第i个样本的真实值，y^i是对第i个样本的预测值。

平均绝对误差MAE（Mean Absolute Error）又被称为L1范数损失（L1l1-norm loss）：
 
 
1.2.2平均平方误差
平均平方误差MSE（Mean Squared Error）又被称为L2范数损失（L2l2-norm loss）：
 
1.2.3 决定系数 --R2分数
决定系数（Coefficient of determination）又被称为R2分数： 
 
如果这个模型不好,R2分数将接近于0 ,因为这两个误差将很接近.
如果模型较好,那么模型的均方误差比简单模型的均方误差小很多R2分数将接近于1
 
1.3	评价指标—聚类
1.3.1兰德指数
兰德指数（Rand index）需要给定实际类别信息C，假设K是聚类结果，a表示在C与K中都是同类别的元素对数，b表示在C与K中都是不同类别的元素对数，则兰德指数为：
  
 
1.3.2轮廓系数
轮廓系数适用于实际类别信息未知的情况。
对于单个样本，设a是与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离，轮廓系数为：
 
对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。
轮廓系数取值范围是[−1,1]，同类别样本距离越相近且不同类别样本距离越远，分数越高
1.4 模型选择
1.4.1 错误类型
1.过渡简化 ---欠拟合
训练集的拟合不够好, 称这种类型为偏差引起的误差 高偏差误差或者欠拟合
2. 过渡复杂化模型，模型太过具体-----过拟合
训练集中的表现良好，但是它趋向于记住数据，而不是学习数据特点 同时测试集的效果很差,称这种类型为方差引起的误差 高方差误差或者过拟合

3. 模型复杂度图表
如何检测错误类型？将训练和测试误差绘制到表上
 
1.4.2  K折交叉验证
训练集用来训练参数，交叉验证集用来对模型做出决定，例如多项式的参数 测试集用于对模型的最终测试
K 折交叉验证--可以预防过拟合
过程:
将训练集数据分成K个包，每次选择一个不同的包用作测试集，剩下的k-1个包用作训练集，然后将模型培训K次，交叉验证重复K次，得出k个测试分数再求平均值，得到最终的效果估测。
 
如果要随机化数据,增加参数shuffle=True, kf=KFold(12,3,shuffle=True)
1.4.3 学习曲线
通过学习曲线检测过拟合和欠拟合
其中一个模型会过拟合，一个欠拟合，还有一个正常。

将使用函数 learning_curve：
train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))
主要参数：
estimator，是我们针对数据使用的实际分类器，例如 LogisticRegression() 或 GradientBoostingClassifier()。
X 和 y 是我们的数据，分别表示特征和标签。
train_sizes 是用来在曲线上绘制每个点的数据大小。
train_scores 是针对每组数据进行训练后的算法训练得分。
test_scores 是针对每组数据进行训练后的算法测试得分。
训练和测试得分是一个包含 3 个值的列表，这是因为函数使用了 3 折交叉验证。
非常重要：可以看出，我们使用训练和测试误差来定义我们的曲线，而这个函数使用训练和测试得分来定义曲线。二者是相反的，因此误差越高，得分就越低。因
此，当你看到曲线时，你需要自己在脑中将它颠倒过来，以便与上面的曲线对比。
这里，我们将对比三个模型：
逻辑回归模型。 决策树模型。 支持向量机模型，具有 RBF 内核，γ 参数为 1000（稍后我们将了解它们的含义）。
from sklearn.model_selection import learning_curve
import pandas as pd
data = pd.read_csv('data.csv')
import numpy as np
X = np.array(data[['x1', 'x2']])
y = np.array(data['y'])

# Fix random seed
np.random.seed(55)
"""
seed( ) 用于指定随机数生成时所用算法开始的整数值。 
1.如果使用相同的seed( )值，则每次生成的随即数都相同； 
2.如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 
3.设置的seed()值仅一次有效
"""
### Imports
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
def randomize(X, Y):
    permutation = np.random.permutation(Y.shape[0])
    """
    shuffle与permutation的区别

函数shuffle与permutation都是对原来的数组进行重新洗牌（即随机打乱原来的元素顺序）；
区别在于shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值。
而permutation不直接在原来的数组上进行操作，
而是返回一个新的打乱顺序的数组，并不改变原来的数组。
    """
    X2 = X[permutation,:]
    Y2 = Y[permutation]
    return X2, Y2
X2, y2 = randomize(X, y)

def draw_learning_curves(X, y, estimator, num_trainings):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()
    plt.title("Learning Curves")
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    plt.plot(train_scores_mean, 'o-', color="g",
             label="Training score")
    plt.plot(test_scores_mean, 'o-', color="y",
             label="Cross-validation score")
    plt.legend(loc="best")
    plt.show()
# TODO: Uncomment one of the three classifiers, and hit "Test Run"
# to see the learning curve. Use these to answer the quiz below.
### Logistic Regression
estimator = LogisticRegression()
### Decision Tree
estimator = GradientBoostingClassifier()
### Support Vector Machine
estimator = SVC(kernel='rbf', gamma=1000)
train_sizes, train_scores, test_scores = learning_curve(
    estimator, X, y, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))

 
我们可以根据这些曲线得出结论：
对数几率回归模型的训练和测试得分很低。
决策树模型的训练和测试得分很高。
支持向量机模型的训练得分很高，测试得分很低
由此可以判断，逻辑回归模型欠拟合，支持向量机模型过拟合，决策树正常。
逻辑回归模型使用一条直线，这太简单了。在训练集上的效果不太好，因此欠拟合。 决策树模型使用一个方形，拟合的很好，并能够泛化。因此，该模型效果很好。 支持向量机模型实际上在每个点周围都画了一个小圆圈。它实际上是在记住训练集，无法泛化。因此 过拟合。
小结:
机器学习的过程:
首先训练数据训练一堆模型->然后使用交叉验证数据挑选最佳模型->最后使用测试集数据测试模型是否好
例如训练逻辑回归模型,假设有四个模型,分别是一次一条直线,二次,三次,四次函数 算出
多项式的斜率和系数,使用交叉验证数据计算这些模型的F1得分.选出F1得分最高的模行最后使用从测试集合确保模型效果好
多项式的系数就是超参数
决策树的超参数 一个是深度,树叶,节点等阈值
支持向量机中有一些超参数 例如内核 可以是线性或者多项式 还有γ参数 可以很大很小 那如何选择这两个超参数呢
1.4.4 网格搜索法
 
用于系统的遍历各种参数组合，通过交叉验证确定最佳效果参数，再去测试最佳参数的模型。网格会遍历给定的参数组合（参数是通过for循环的方式进行组合的），这个组合是用户提前给定的。再根据给定的模型自动进行交叉验证，通过调节每一个参数来跟踪评分结果，通过评分确定最佳效果参数得到最优模型
总结：网格搜索法采用基于网格搜索的交叉验证法来选择模型参数，避免了参数选择的盲目性和随意性。 
使用网格搜索法优化学习算法，首先确定参数字典，说明算法，再使用网格搜索法，它会自动生成一个各种参数的‘网格’表格，并对各种参数组合进行尝试，从而返回最佳参数组合，优化算法。
在 sklearn 中的网格搜索:
假设我们想要训练支持向量机，并且我们想在以下参数之间做出决定：
kernel：poly或rbf。 C：0.1,1 或 10。
1.） 导入 GridSearchCV  
from sklearn.model_selection import GridSearchCV
2.）选择参数
现在我们来选择我们想要选择的参数，并形成一个字典。 在这本字典中，键 (keys) 将是参数的名称，值 （values) 将是每个参数可能值的列表。
parameters = {'kernel':['poly', 'rbf'],'C':[0.1, 1, 10]}
3.）创建一个评分机制 (scorer)
需要确认将使用什么指标来为每个候选模型评分。 这里，我们将使用 F1 分数。
from sklearn.metrics import make_scorer
from sklearn.metrics import f1_score
scorer = make_scorer(f1_score)
4.)	使用参数 (parameter) 和评分机制 (scorer) 创建一个 GridSearch 对象。 使用此对象与数据保持一致 （fit the data) 。
# Create the object.
grid_obj = GridSearchCV(clf, parameters, scoring=scorer)
# Fit the data
grid_fit = grid_obj.fit(X, y)
5.)	获得最佳估算器 (estimator)
现在你可以使用这一估算器best_clf来做出预测。
best_clf = grid_fit.best_estimator_
1.5 特征工程之特征选择
1.5.1 特征来源
在做数据分析的时候，特征的来源一般有两块，一块是业务已经整理好各种特征数据，需要去找出适合问题需要的特征；另一块是从业务特征中自己去寻找高级数据特征。 
首先看当业务已经整理好各种特征数据时，如何去找出适合问题需要的特征，此时特征数可能成百上千，哪些才是我们需要的呢？
第一步是找到该领域懂业务的专家，让他们给一些建议。比如我们需要解决一个药品疗效的分类问题，那么先找到领域专家，向他们咨询哪些因素（特征）会对该药品的疗效产生影响，较大影响的和较小影响的都要。这些特征就是我们的特征的第一候选集。
最简单的方法就是方差筛选。方差越大的特征，那么可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对模型训练没有任何作用，可以直接舍弃。在实际应用中，会指定一个方差的阈值，当方差小于这个阈值的特征会被筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。
特征选择方法有很多，一般分为三类：第一类过滤法比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面提到的方差筛选就是过滤法的一种。第二类是包装法，根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。第三类嵌入法则稍微复杂一点，它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。
1.5.2 特种选择之过滤法
除了特征的方差这第一种方法，还有其他一些统计学指标可以使用。
第二个可以使用的是相关系数。这个主要用于输出连续值的监督学习算法中。分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征。
第三个可以使用的是假设检验，比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。个人觉得它比比粗暴的方差法好用。在sklearn中，可以使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，可以给定卡方值阈值， 选择卡方值较大的部分特征。
除了卡方检验，还可以使用F检验和t检验，它们都是使用假设检验的方法，只是使用的统计分布不是卡方分布，而是F分布和t分布而已。在sklearn中，有F检验的函数f_classif和f_regression，分别在分类和回归特征选择时使用。
第四个是互信息，即从信息熵的角度分析各个特征和输出值之间的关系评分。在决策树算法中讲到过互信息（信息增益）。互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。在sklearn中，可以使用mutual_info_classif(分类)和mutual_info_regression(回归)来计算各个输入特征和输出值之间的互信息。
以上就是过滤法的主要方法，个人经验是，在没有什么思路的 时候，可以优先使用卡方检验和互信息来做特征选择
1.5.3 特征选择之包装法
包装法的解决思路是选择一个目标函数来一步步的筛选特征。
最常用的包装法是递归消除特征法(简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。在sklearn中，可以使用RFE函数来选择特征。
  1.5.4 特征选择之嵌入法
嵌入法也是用机器学习的方法来选择特征，但是它和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。在sklearn中，使用SelectFromModel函数来选择特征。
最常用的是使用L1正则化和L2正则化来选择特征。正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 但是会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，选择特征系数较大的特征。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归，也可以使用决策树或者GBDT。 
1.5.5 特征选择之高级特征的选择
拿到已有的特征后，还可以根据需要寻找到更多的高级特征。比如有车的路程特征和时间间隔特征，就可以得到车的平均速度这个二级特征。根据车的速度特征，就可以得到车的加速度这个三级特征，根据车的加速度特征，就可以得到车的加加速度这个四级特征。。。也就是说，高级特征可以一直寻找下去。
寻找高级特征最常用的方法有：
若干项特征加和： 假设希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。
个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。
特征选择是特征工程的第一步，它关系到机器学习算法的上限。因此原则是尽量不错过一个可能有用的特征，但是也不滥用太多的特征。
1.6 特征工程之特征表达
特征表达 ，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。
1.6.1 缺失值处理
首先会看是该特征是连续值还是离散值。
1)	连续值：
两种选择：
a)	选择所有有该特征值的样本，然后取平均值，来填充缺失值
b)	取中位数来填充缺失值
2)	离散值
一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值。
在sklearn中，可以使用preprocessing.Imputer来选择这三种不同的处理逻辑做预处理。
1.6.2 特殊的特征处理  
时间原始特征　
第一种是使用连续的时间差值法，即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值。
第二种方法是根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用。 
第三种是权重法，即根据时间的新旧得到一个权重值。比如对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三天前的设置一个较大的权重。当然，还有其他的设置权重的方法，这个要根据要解决的问题来灵活确定。
地理特征
比如“广州市天河区XX街道XX号”
如果是处理成离散值，则需要转化为多个离散特征，比如城市名特征，区县特征，街道特征等。但是如果需要判断用户分布区域，则一般处理成连续值会比较好，这时可以将地址处理成经度和纬度的连续特征。
1.6.3离散特征的连续化处理
线性回归，逻辑回归等算法只能处理连续值特征，不能处理离散值特征， 这时可以将离散特征连续化处理。
1.离散特征连续化的处理方法是独热编码one-hot encoding。
处理方法其实比较简单，比如某特征的取值是高，中和低，那么就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。sklearn的OneHotEncoder可以帮我们做这个处理。
2.特征嵌入embedding。
这个一般用于深度学习中。比如对于用户的ID这个特征，如果要使用独热编码，则维度会爆炸，如果使用特征嵌入就维度低很多了。对于每个要嵌入的特征，会有一个特征嵌入矩阵，这个矩阵的行很大，对应该特征的数目。比如用户ID，如果有100万个，那么嵌入的特征矩阵的行就是100万。但是列一般比较小，比如可以取20。这样每个用户ID就转化为了一个20维的特征向量。进而参与深度学习模型。在tensorflow中，可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用tf.nn.embedding_lookup找到该用户的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。
此外，在自然语言处理中，也可以用word2vec将词转化为词向量，进而可以进行一些连续值的后继处理。
1.6.4离散特征的离散化处理
 离散特征有时间也不能直接使用，需要先进行转化。比如最常见的，如果特征的取值是高，中和低，那么就算你需要的是离散值，也是没法直接使用的。
对于原始的离散值特征，最常用的方法也是独热编码
第二种方法是虚拟编码dummy coding，它和独热编码类似，但是它的特点是，如果特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代替。比如一个特征的取值是高，中和低，那么只需要两位编码，比如只编码中和低，如果是1，0，则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热编码广，因此一般有需要的话还是使用独热编码比较好。
对于分类问题的特征输出，一般需要用sklearn的LabelEncoder将其转化为0,1,2，...这样的类别标签值。
1.6.5连续特征的离散化处理
对于连续特征，有时候也可以将其做离散化处理。这样特征变得高维稀疏，方便一些算法的处理。
1.	根据阈值进行分组
比如根据连续值特征的分位数，将该特征分为高，中和低三个特征。将分位数从0-0.3的设置为高，0.3-0.7的设置为中，0.7-1的设置为高。
2.	使用GBDT
在LR+GBDT的经典模型中，就是使用GDBT来先将连续值转化为离散值。那么如何转化呢？比如用训练集的所有连续值和标签输出来训练GBDT，最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，在第二颗决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。在sklearn中，可以用GradientBoostingClassifier的 apply方法很方便的得到样本离散化后的特征，然后使用独热编码即可。 
1.7 特征工程之特征预处理
1.7.1 标准化和归一化
1.z-score标准化
这是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做 z-score标准化。具体的方法是求出样本特征x的均值mean和标准差std，然后用（x-mean)/std来代替原特征。这样特征就变成了均值为0，方差为1了。在sklearn中，可以用StandardScaler来做z-score标准化。当然，如果是用pandas做数据预处理，可以自己在数据框里面减去均值，再除以方差，自己做z-score标准化。    
2.max-min标准化
也称为离差标准化，预处理后使特征值映射到[0,1]之间。具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。如果希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单。用(x-min)(b-a)/(max-min)+a来代替原特征即可。在sklearn中， 可以用MinMaxScaler来做max-min标准化。这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。
3.L1/L2范数标准化
如果只是为了统一量纲，那么通过L2范数整体标准化也是可以的，具体方法是求出每个样本特征向量x⃗ 的L2范数||x⃗ ||2,然后用x⃗ /||x⃗ ||2代替原样本特征即可。当然L1范数标准化也是可以的，即用x⃗ /||x⃗ ||1代替原样本特征。通常情况下，范数标准化首选L2范数标准化。在sklearn中可以用Normalizer来做L1/L2范数标准化。
　此外，经常还会用到中心化，主要是在PCA降维的时候，此时求出特征x的平均值mean后，用x-mean代替原特征，也就是特征的均值变成了0, 但是方差并不改变。这个很好理解，因为PCA就是依赖方差来降维的。
　虽然大部分机器学习模型都需要做标准化和归一化，也有不少模型可以不做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进。
1.7.2 异常特征的样本清洗
在实际项目中拿到的数据往往有不少异常数据，有时候不筛选出这些异常数据很可能让后面的数据分析模型有很大的偏差。如何筛选出这些异常特征样本呢？常用的方法有两种。
　第一种是聚类，比如可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。可以将其从训练集过滤掉。
　第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。
　当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止将正常的样本过滤掉了。
1.7.3 处理不平衡的数据
　 做分类算法训练时，如果训练集里的各个类别的样本数量不是大约相同的比例，就需要处理样本不平衡问题。如果不处理，那么拟合出来的模型对于训练集中少样本的类别泛化能力会很差。
举个例子，一个二分类问题，如果训练集里A类别样本占90%，B类别样本占10%。 而测试集里A类别样本占50%， B类别样本占50%， 如果不考虑类别不平衡问题，训练出来的模型对于类别B的预测准确率会很低，甚至低于50%。
　如何解决这个问题呢？一般是两种方法：权重法或者采样法。
1.	权重法
是比较简单的方法，可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。
　如果权重法做了以后发现预测效果还不好，可以考虑采样法。
2.	采样法
常用的也有两种思路，一种是对类别样本数多的样本做子采样, 比如训练集里A类别样本占90%，B类别样本占10%。那么可以对A类的样本子采样，直到子采样得到的A类样本数和B类别现有样本一致为止，这样就只用子采样得到的A类样本数和B类现有样本一起做训练集拟合模型。第二种思路是对类别样本数少的样本做过采样, 还是上面的例子，对B类别的样本做过采样，直到过采样得到的B类别样本数加上B类别原来样本一起和A类样本数一致，最后再去拟合模型。
　　　　上述两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。所以有的算法就通过其他方法来避免这个问题，如下：
3.	SMOTE算法通过人工合成的方法来生成少类别的样本
1)	对于某一个缺少样本的类别，它会随机找出几个该类别的样本
2)	再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合
3)	然后在这个集合中不停的选择距离较近的两个样本，
4)	在这两个样本之间，比如中点，构造一个新的该类别样本。
举个例子，比如该类别的候选合成集合有两个样本(x1,y),(x2,y)那么SMOTE采样后，可以得到一个新的训练样本  
通过这种方法， 可以得到不改变训练集分布的新样本，让训练集中各个类别的样本数趋于平衡。可以用imbalance-learn这个Python库中的SMOTEENN类来做SMOTE采样。

二  监督学习
2.1 线性回归
2.1.1 函数模型
当该曲线为一条直线时，就是线性回归。其中，“线性”是指特征和结果都是线性的

 
 
2.1.2 损失函数
一般线性回归我们用均方误差作为损失函数
 
矩阵:
 
这就是损失函数，求解出一个使得损失函数最小的W,求解的方法有最小二乘法、梯度下降法等。
帮助某个点靠近直线:
1.	绝对值技巧
学习率是α 直线是y=w1x+w2 直线上方存在点是(p,q)
绝对值技巧是 斜率 变为 w1+α.p 截距变为 w2+α.1
新方程是 y=( w1+α.p)x+( w2+α)
如果是直线下方点(p,q) y=( w1-α.p)x+( w2-α)
2.	平方技巧
学习率是α 直线是y=w1x+w2 直线上方存在点是(p,q)
点到直线的垂直距离 为 q-y 因为Y是直线的值,q是这个点的纵坐标
新方程是 y=( w1+α.p(q-y))x+( w2+α(q-y))
如果是直线下方点(p,q) y=( w1-α.p(q-y))x+( w2-α(q-y))
3. 平均绝对误差和均方误差
直线上方点(x,y) 预测值为 直线y~ 直线上的点位(x,y~) 也就是说 上方点到直线上的垂直距离为y-y~ 这就是我们所说的误差 下方点(x,y)时 距离是y~-y 取正数 距离绝对值. 误差总值是 第i个点到m个 |Yi-Yi~| 相加 那么平均绝对误差就是 乘 1/m,见截图"平均绝对误差"
m是数据集合中的总点数
 
 
2.1.2.1 最小二乘法
 
代数法解法
 
 
矩阵满秩可求解时（求导等于0）:
 
 
 
 
2.1.2.2  梯度下降法
矩阵不满秩时（梯度下降）
利用沿着梯度下降最快的方向求偏导数，得到损失函数的全局最小值时的参数w. 
在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。
　从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数 是凸函数，梯度下降法得到的解就一定是全局最优解。

1.步长即学习率：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。
2.特征：指的是样本中输入部分，比如2个单特征的样本（x(0),y(0)）,（x(1),y(1)）,则第一个样本特征为x(0)，第一个样本输出为y(0)。
3.预测函数：在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)。比如对于单个特征的m个样本（x(i),y(i)）(i=1,2,...m),可以采用拟合函数如下： hθ(x)=θ0+θ1x。
4.损失函数（loss function）为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于m个样本（xi,yi）(i=1,2,...m),采用线性回归，损失函数为：
 
其中xi表示第i个样本特征，yi表示第i个样本对应的输出，hθ(xi)为预测函数。   
梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示
代数方式算法描述:
先决条件:
 确认优化模型的预测函数和损失函数。
比如对于线性回归，预测函数表示为: 
hθ(x1,x2,...xn)=θ0+θ1x1+...+θnxn
其中θi (i = 0,1,2... n)为模型参数，xi  (i = 0,1,2... n)为每个样本的n个特征值。
对应于上面的假设函数，损失函数为：
 
矩阵:
 
算法相关参数初始化:
主要是初始化θ0,θ1,...,θn,算法终止距离ε 以及步长α 。在没有任何先验知识的时候，我喜欢将所有的θ 初始化为0， 将步长初始化为1。在调优的时候再优化。
算法过程:
1.	确定当前位置的损失函数的梯度,对于θi,其梯度表达式如下：
 
由于 
所以算法过程步骤1中对于θi的偏导数计算如下
 
2.	用步长乘以损失函数的梯度，得到当前位置下降的距离
 
3.	确定是否所有的θi,梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的θi(i=0,1,...n)即为最终结果。否则进入步骤4.
4.	更新所有的θ，对于θi，其更新表达式如下。更新完毕后继续转入步骤1.
 
 
矩阵:
 
 
梯度下降的算法调优:
1.算法的步长选择
在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。
2.算法参数的初始值选择
初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。
3.归一化
由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的期望x¯和标准差std(x)，然后转化为：
 
批量梯度下降法: 
之前的就是批量梯度下降法
 
由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。
随机梯度下降法: 
在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度, 对应的更新公式是：
 
对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解
小批量梯度下降法:
于m个样本，我们采用x个样子来迭代，1<x<m
一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式:
 
2.1.2.3	梯度下降法与最小二乘
梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。
2.1.3	scikit-learn 中的线性回归
2.1.3.1  LinearRegression
损失函数：
LinearRegression类就是我们平时说的最常见普通的线性回归，它的损失函数也是最简单的，如下：
　 
损失函数的优化方法：
对于这个损失函数，一般有梯度下降法和最小二乘法两种极小化损失函数的优化方法，而scikit中的LinearRegression类用的是最小二乘法。通过最小二乘法，可以解出线性回归系数θ为：
 
验证方法：
LinearRegression类并没有用到交叉验证之类的验证方法，需要我们自己把数据集分成训练集和测试集，然后训练优化。
使用场景：
一般来说，只要我们觉得数据有线性关系，LinearRegression类是我们的首先。如果发现拟合或者预测的不好，再考虑用其他的线性回归库。如果是学习线性回归，推荐先从这个类开始第一步的研究。

对于你的线性回归模型，你将使用 scikit-learn 的 LinearRegression 类。此类会提供函数 fit() 来将模型与数据进行拟合。
>>> from sklearn.linear_model import LinearRegression
>>> model = LinearRegression()
>>> model.fit(x_values, y_values)
model 变量是拟合到数据 x_values 和 y_values 的线性回归模型。拟合模型意味着寻找拟合训练数据的最佳线条。我们使用模型的 predict() 函数做出两个预测。
>>> print(model.predict([ [127], [248] ]))
[[ 438.94308857, 127.14839521]]
该模型返回了一个预测数组，每个输入数组一个预测结果。第一个输入 [127] 的预测结果是 438.94308857。第二个输入 [248] 的预测结果是 127.14839521。用 [127] 这样的数组（而不只是 127）进行预测的原因是模型可以使用多个特征进行预测。我们将在这节课的稍后部分讲解如何在线性回归中使用多个变量。暂时先继续使用一个值。

2.1.3.2  Ridge
损失函数：　
　由于LinearRegression没有考虑过拟合的问题，有可能泛化能力较差，这时损失函数可以加入正则化项，如果加入的是L2范数的正则化项，这就是Ridge回归。损失函数如下
 
Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，不至于过拟合。
损失函数的优化方法：
对于这个损失函数，一般有梯度下降法和最小二乘法两种极小化损失函数的优化方法，而scikit中的Ridge类用的是最小二乘法。通过最小二乘法，可以解出线性回归系数θ为：
 
其中E为单位矩阵。
验证方法：
Ridge类并没有用到交叉验证之类的验证方法，需要我们自己把数据集分成训练集和测试集，需要自己设置好超参数α。然后训练优化。
使用场景：
一般来说，只要我们觉得数据有线性关系，用LinearRegression类拟合的不是特别好，需要正则化，可以考虑用Ridge类。但是这个类最大的缺点是每次我们要自己指定一个超参数α，然后自己评估α的好坏，比较麻烦，一般我都用下一节讲到的RidgeCV类来跑Ridge回归，不推荐直接用这个Ridge类，除非你只是为了学习Ridge回归。
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1)
ridge.fit(X_train, y_train)

2.1.3.3  RidgeCV
RidgeCV类的损失函数和损失函数的优化方法完全与Ridge类相同，区别在于验证方法。
验证方法：
RidgeCV类对超参数α使用了交叉验证，来帮忙我们选择一个合适的α。在初始化RidgeCV类时候，我们可以传一组备选的α值，10个，100个都可以。RidgeCV类会帮我们选择一个合适的α。免去了我们自己去一轮轮筛选α的苦恼。　　
使用场景：
一般来说，只要我们觉得数据有线性关系，用LinearRegression类拟合的不是特别好，需要正则化，可以考虑用RidgeCV类。不是为了学习的话就不用Ridge类。为什么这里只是考虑用RidgeCV类呢？因为线性回归正则化有很多的变种，Ridge只是其中的一种。所以可能需要比选。如果输入特征的维度很高，而且是稀疏线性关系的话，RidgeCV类就不合适了。这时应该主要考虑下面几节要讲到的Lasso回归类家族。
from sklearn.linear_model import RidgeCV
ridgecv = RidgeCV(alphas=[0.01, 0.1, 0.5, 1, 3, 5, 7, 10, 20, 100])
ridgecv.fit(X_train, y_train)
ridgecv.alpha_ 
输出结果为：7.0，说明在我们给定的这组超参数中， 7是最优的α值。

通过Ridge回归的损失函数表达式可以看到，α越大，那么正则项惩罚的就越厉害，得到回归系数θ就越小，最终趋近与0。而如果α越小，即正则化项越小，那么回归系数θ就越来越接近于普通的线性回归系数。
2.1.3.3   Lasso
损失函数：
线性回归的L1正则化通常称为Lasso回归，它和Ridge回归的区别是在损失函数上增加了的是L1正则化的项，而不是L2正则化项。L1正则化的项也有一个常数系数α来调节损失函数的均方差项和正则化项的权重，具体Lasso回归的损失函数表达式如下：
 Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0。增强模型的泛化能力。
损失函数的优化方法：
Lasso回归的损失函数优化方法常用的有两种，坐标轴下降法和最小角回归法。　　　　
验证方法：
Lasso类并没有用到交叉验证之类的验证方法，和Ridge类类似。需要我们自己把数据集分成训练集和测试集，需要自己设置好超参数αα。然后训练优化。
使用场景：
一般来说，对于高维的特征数据，尤其线性关系是稀疏的，我们会采用Lasso回归。或者是要在一堆特征里面找出主要的特征，那么Lasso回归更是首选了。但是Lasso类需要自己对α调优，所以不是Lasso回归的首选，一般用到的是下一节要讲的LassoCV类。
2.1.3.3   LassoCV
LassoCV类的损失函数和损失函数的优化方法完全与Lasso类相同，区别在于验证方法。
验证方法：
LassoCV类对超参数α使用了交叉验证，来帮忙我们选择一个合适的α。在初始化LassoCV类时候，我们可以传一组备选的α值，10个，100个都可以。LassoCV类会帮我们选择一个合适的α。免去了我们自己去一轮轮筛选α的苦恼。　
使用场景：　　
LassoCV类是进行Lasso回归的首选。当我们面临在一堆高位特征中找出主要特征时，LassoCV类更是必选。当面对稀疏线性关系时，LassoCV也很好用。

2.1.3.1 线性回归注意事项
线性回归隐含一系列前提假设，并非适合所有情形，因此应当注意以下两个问题
1.	最适用于线性数据
线性回归会根据训练数据生成直线模型。如果训练数据包含非线性关系，你需要选择：调整数据（进行数据转换）、增加特征数量（参考下节内容）或改用其他模型。
2.容易受到异常值影响
线性回归的目标是求取对训练数据而言的 “最优拟合” 直线。如果数据集中存在不符合总体规律的异常值，最终结果将会存在不小偏差。
在大多数情况下，模型需要基本上能与大部分数据拟合，所以要小心异常值！
2.1.4 正则化
 -----保证模型不会过拟合,可以用于回归或者分类
正常情况下都会选择误差更小的模型,但是过拟合的模型误差更小,想办法更大误差,而不被选择,因为我们更想要简单的模型更具有通用性,而不是复杂模型无法泛化
L1正则化
将权重绝对值相加 不包括偏差 乘以常数系数C
L2正则化
将权重的平方相加,不包括偏差的平方 乘以常数系数C
只有数据稀疏时,L1比L2正则化更快 提供特征选择 将不相关数据转化成0 而l2不行
l2更适用于均匀数据
2.1.5 逻辑回归
逻辑回归是一个分类算法，它可以处理二元分类以及多元分类
线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。Y是连续的，所以是回归模型。
如果我们想要Y是离散的话，怎么办呢？就是逻辑回归了
对于这个Y再做一次函数转换，变为g(Y) 。如果我们令g(Y) 的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。 
对线性回归的结果做一个在函数g上的转换，可以变化为逻辑回归。这个函数g在逻辑回归中我们一般取为sigmoid函数，形式如下：
 
当z趋于正无穷时，g(z)趋于1，而当z趋于负无穷时
g(z)趋于0，这非常适合于我们的分类概率模型。
有一个很好的导数性质：
 

2.1.5.1二元逻辑回归的损失函数
线性回归的损失函数，由于线性回归是连续的，所以可以使用模型误差的的平方和来定义损失函数,但是逻辑回归不是连续的,不可以用,但是可以用最大似然估计法来推导损失函数
假设我们的样本输出是0或者1两类。那么我们有：
 
 把这两个式子写成一个式子，就是：

 
其中y的取值只能是0或者1。
用矩阵法表示即为:
 
得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数θ。
用对数似然函数最大化，对数似然函数取反即为我们的损失函数J(θ)。
似然函数的代数表达式为：
 
其中m为样本的个数。
对似然函数对数化取反的表达式，即损失函数表达式为：
 

 

2.1.5.2二元逻辑回归的损失函数的优化方法
　　对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见是梯度下降法，坐标轴下降法，等牛顿法等。这里推导出梯度下降法中θ每次迭代的公式。由于代数法推导比较的繁琐，我习惯于用矩阵法来做损失函数的优化过程，这里给出矩阵法推导二元逻辑回归梯度的过程。
 
 
其中，α为梯度下降法的步长。
2.1.5.2二元逻辑回归的正则化
逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化。
逻辑回归的L1正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数α作为惩罚系数，调节惩罚项的大小。
二元逻辑回归的L1正则化损失函数表达式如下：
 
 
2.1.5.3  scikit-learn 逻辑回归类
与逻辑回归有关的主要是这3个类:LogisticRegression， LogisticRegressionCV 
logistic_regression_path。
其中LogisticRegression和LogisticRegressionCV的主要区别是LogisticRegressionCV使用了交叉验证来选择正则化系数C。
而LogisticRegression需要自己每次指定一个正则化系数。除了交叉验证，以及选择正则化系数C以外， LogisticRegression和LogisticRegressionCV的使用方法基本相同。
正则化选择参数：penalty
LogisticRegression和LogisticRegressionCV默认就带了正则化项。
penalty参数可选择的值为"l1"和"l2".分别对应L1的正则化和L2的正则化，默认是L2的正则化。
在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。
优化算法选择参数：solver
a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。

penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。
但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。
这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。
郁闷的是liblinear只支持OvR，不支持MvM，这样如果我们需要相对精确的多元逻辑回归时，就不能选择liblinear了。也意味着如果我们需要相对精确的多元逻辑回归不能使用L1正则化了。
分类方式选择参数：multi_class
multi_class参数决定了我们分类方式的选择，有 ovr和multinomial两个值可以选择，默认是 ovr。
Ovr 无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。
如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg, lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。
MvM
MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类
如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg, lbfgs和sag都可以选择。
但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。
类型权重参数： class_weight
使用情况: 是误分类的代价很高。考虑这个参数
class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。
如果class_weight选择balanced，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。
样本权重参数： sample_weight
使用情况: 是样本是高度失衡的 考虑这个参数
由于样本不平衡，导致样本不是总体样本的无偏估计，从而可能导致我们的模型预测能力下降。遇到这种情况，我们可以通过调节样本权重来尝试解决这个问题。调节样本权重的方法有两种，第一种是在class_weight使用balanced。第二种是在调用fit函数时，通过sample_weight来自己调节每个样本权重。
2.2 朴素贝叶斯
朴素贝叶斯方法，是指
朴素：特征条件独立
贝叶斯：基于贝叶斯定理
在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。
2.2.1 贝叶斯公式
 
条件概率 P(B|A)  事件A发生下B发生的概率
联合概率：多个事件同时发生的可能性,前提是事件是相互独立的互不影响，如果不独立则联合概率为P(AB) = P(A)P(B|A)
当P(B) = P(B|A)时表示事件是相互独立的
P(A) P(B)就是先验概率
 
  
2.2.2  scikit-learn 实现朴素贝叶斯
在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。
GaussianNB就是先验为高斯分布的朴素贝叶斯
MultinomialNB就是先验为多项式分布的朴素贝叶斯
BernoulliNB就是先验为伯努利分布的朴素贝叶斯。

如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。
如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。
如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。
2.2.2.1  GaussianNB类使用总结
1.GaussianNB假设特征的先验概率为正态分布，即如下式：
 
 
 
2.	GaussianNB类的主要参数仅有一个，即先验概率priors ，对应Y的各个类的先验概率P(Y=Ck)。这个值默认不给出，如果不给出此时P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。如果给出的话就以priors 为准。
3.	使用GaussianNB的fit方法拟合数据后，我们可以进行预测。
 
predict方法就是我们最常用的预测方法，直接给出测试集的预测类别输出。
predict_proba则不同，它会给出测试集样本在各个类别上预测的概率。容易理解，predict_proba预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到类别。
predict_log_proba和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化。转化后predict_log_proba预测出的各个类别对数概率里的最大值对应的类别，也就是predict方法得到类别。

import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
#拟合数据
clf.fit(X, Y)
print "==Predict result by predict=="
print(clf.predict([[-0.8, -1]]))
print "==Predict result by predict_proba=="
print(clf.predict_proba([[-0.8, -1]]))
print "==Predict result by predict_log_proba=="
print(clf.predict_log_proba([[-0.8, -1]]))
==Predict result by predict==
[1]
==Predict result by predict_proba==
[[  9.99999949e-01   5.05653254e-08]]
==Predict result by predict_log_proba==
[[ -5.05653266e-08  -1.67999998e+01]]
从上面的结果可以看出，测试样本[-0.8,-1]的类别预测为类别1。具体的测试样本[-0.8,-1]被预测为1的概率为9.99999949e-01 ，远远大于预测为2的概率5.05653254e-08。这也是为什么最终的预测结果为1的原因了。
GaussianNB一个重要的功能是有 partial_fit方法，这个方法的一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集，非常方便
2.2.2.2  MultinomialNB类使用总结
MultinomialNB假设特征的先验概率为多项式分布，即如下式：

 
MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个

参数alpha即为上面的常数λ，如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数。

布尔参数fit_prior表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率。

否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m
。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。总结如下：
 
在使用MultinomialNB的fit方法或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。由于方法和GaussianNB完全一样，这里就不累述了。

使用 sklearns 的 sklearn.naive_bayes 方法对我们的数据集做出预测。

from sklearn.naive_bayes import MultinomialNB
naive_bayes = MultinomialNB()
naive_bayes.fit(training_data, y_train)

其中:
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)

predictions = naive_bayes.predict(testing_data)
2.2.2.3  BernoulliNB类使用总结
　BernoulliNB假设特征的先验概率为二元伯努利分布，即如下式：

 BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同。唯一增加的一个参数是binarize。这个参数主要是用来帮BernoulliNB处理二项分布的，可以是数值或者不输入。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类。
在使用BernoulliNB的fit或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。由于方法和GaussianNB完全一样

2.3  感知器
分类：把数据划分成不同的类别 回归：建立数据间的连续关系
一个神经元可以看作是将一个或多个输入处理成一个输出的计算单元。一个感知器函数类似于一个神经元；它接受一个或多个输入，处理他们然后返回一个输出。
每个输入单元分别代表一个特征

感知机，就是二类分类的线性分类模型，其输入为样本的特征向量，输出为样本的类别，取+1和-1二值，即通过某样本的特征，就可以准确判断该样本属于哪一类。顾名思义，感知机能够解决的问题首先要求特征空间是线性可分的，再者是二类分类，即将样本分为{+1, -1}两类。从比较学术的层面来说，由输入空间到输出空间的函数：

使用感知机一个最大的前提，就是数据是线性可分的(找不到直线或者平面进行分类即为线性不可分)。这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。
2.3.1 感知机模型
如果我们有m个样本，每个样本对应于n维特征和一个二元类别输出，如下：
 
让其中一种类别的样本都满足:
 
让另一种类别的样本都满足:
 
从而得到线性可分。如果数据线性可分，这样的超平面一般都不是唯一的，也就是说感知机模型可以有多个解。
 
输入权值 一个感知器可以接收多个输入，每个输入上有一个权值，此外还有一个偏置项，就是上图中的。w和b为感知机参数，w为权值（weight），b为偏置（bias）。
激活函数: 得到输出的类标
感知器的激活函数可以有很多选择，比如我们可以选择下面这个阶跃函数来作为激活函数：
sign为符号函数：单位阶跃函数”>=0 取1 其他取-1
 
输出: 由下面这个公式来计算, f就是激活函数
 
单层感知怎么求解呢？
 
这里面的损失函数应该是交叉嫡损失函数,再进行优化得到的迭代公式

感知器（Perceptron） 是一种用于线性可分数据集的二类分类器算法。这种算法的局限性很大:只能将数据分为 2 类 ,数据必须是线性可分的
 

感知器步骤如下所示。对于坐标轴为 (p,q) (p,q) 的点，标签 y，以及等式 y~= step(w1x1 + w2x2 + b) 给出的预测
如果点分类正确，则什么也不做
如果预测点分类为正，但是实际标签为负，则分别减去 αp,αq, 和 α 至 w1, w2, b
如果预测点分类为负，但是实际标签为正，则分别将 αp,αq, 和 α 加到 w1, w2, b 上
请随意改动算法的参数（epoch 数量、学习速率，甚至随机化初始参数），看看初始条件对解决方案有何影响！

2.4  决策树
决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。
决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。
决策树节点停止分裂的一般性条件：
（1）最小节点数
　　当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。
（2）熵或者基尼值小于阀值
     由上述可知，熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。
（3）决策树的深度达到指定的条件
节点的深度可以理解为节点与决策树跟节点的距离，如根节点的子节点的深度为1，因为这些节点与跟节点的距离为1，子节点的深度要比父节点的深度大1。决策树的深度是所有叶子节点的最大深度，当深度到达指定的上限大小时，停止分裂。
（4）所有特征已经使用完毕，不能继续进行分裂。
     被动式停止分裂的条件，当已经没有可分的属性时，直接将当前节点设置为叶子节点。
根据决策树的输出结果，决策树可以分为分类树和回归树，分类树输出的结果为具体的类别，而回归树输出的结果为一个确定的数值。
决策树的构建算法主要有ID3、C4.5、CART三种，其中ID3和C4.5是分类树，CART是分类回归树，将在本系列的ID3、C4.5和CART中分别讲述。
ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率作为不纯度；CART算法使用基尼系数作为不纯度。
2.4.1 信息论基础
熵度量了事物的不确定性，越不确定的事物，它的熵就越大。X分布得越离散，H(X)的值越高,具体的，随机变量X的熵的表达式如下：
 
其中n代表X的n种不同的离散取值。而pi代表了X取值为i的概率，log为以2或者e为底的对数
比如X有2个可能的取值，而这两个取值各为1/2时X的熵最大，此时X具有最大的不确定性。
 
如果一个值概率大于1/2，另一个值概率小于1/2，则不确定性减少，对应的熵也会减少。比如一个概率1/3，一个概率2/3，则对应熵为
 两个变量X和Y的联合熵表达式：
 

条件熵类似于条件概率,它度量了我们的X在知道Y以后剩下的不确定性
条件熵的表达式H(X|Y):
 
互信息表达式:
I(X,Y)=H(X)-H(X|Y)
是度量两个事件集合之间的相关性
如果X与Y独立，则P(X,Y)=P(X)P(Y)，I(X,Y)就为0，即代表X与Y不相关
如果X,Y关系越密切，I(X,Y)就越大
它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。
左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是我们的互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X)。两个椭圆的并就是H(X,Y)。
 
在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。
2.4.2  ID3算法
ID3算法就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。
例:
我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0.

样本D的熵为： 
 
样本D在特征下的条件熵为： 
 
对应的信息增益为 :
 

算法过程:
输入: m个样本，样本输出集合为D，每个样本有n个离散特征，特征集合即为A;
输出:为决策树T;
1) 初始化信息增益的阈值ϵ
2）判断样本是否为同一类输出Di，如果是则返回单节点树T。标记类别为Di
(y是否有0或者1两个取值两个类还是只有一个取值只有一个类别)
3) 否则判断特征是否为空，如果A=∅,则返回单节点树T，标记类别为样本中输出类别D中实例数最多的类别。(样本中是否有特征A)
4）否则计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag
5) 如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D中实例数最多的类别。
6）否则，按特征Ag的不同取值Agi将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Agi。返回增加了节点的数T。
7）对于所有的子节点，令D=Di，A=A−{Ag}递归调用2-6步，得到子树Ti并返回。
ID3 算法的不足:
a)ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
b)ID3采用信息增益大的特征优先建立决策树的节点。用信息增益作为标准容易偏向于取值较多的特征
在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。
c) ID3算法对于缺失值的情况没有做考虑
d) 没有考虑过拟合的问题
2.4.3  C4.5算法
针对ID3的四个缺点，C 4.5 算法处理如下：
1.	不能处理连续特征。
C4.5的思路是将连续的特征离散化。
比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am。则C4.5取相邻两样本值的平均数，T1=(a1+a2)/2,a1<= T1<=a2…am,一共取得m-1个划分点T，其中第i个划分点Ti表示Ti表示为：
 
对于这m-1个点Ti，分别计算以该点T1…. Tm-1作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。
比如取到的增益最大的点为at,则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。
要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
连续属性的分裂只能二分裂，离散属性的分裂可以多分裂，比较分裂前后信息增益率，选取信息增益率最大的。
2.	信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量IR(X,Y) 它是信息增益和特征熵的比值。用它挑选最优特征
表达式如下：
 
其中D为样本特征输出的集合，A为样本特征，对于特征熵HA(D), 表达式如下：

 
其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。信息增益(比)越大,特征越好
特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。
3. 缺失值处理的问题
主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。
对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。
对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。
4. C4.5引入了正则化系数进行初步的剪枝。 
除了上面的4点，C4.5和ID的思路区别不大。
C4.5算法的不足
C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。
1)	由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。
剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种:
一是预剪枝，即在生成决策树的时候就决定是否剪枝。
二是后剪枝，即先生成决策树，再通过交叉验证来剪枝。
主要采用的是后剪枝加上交叉验证选择最合适的决策树。
2)C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
3)C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
4)C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。
2.4.4  CART算法
C4.5的四个问题在CART树里面部分加以了改进。所以目前如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。
scikit-learn的决策树使用的也是CART算法。
CART算法可以做回归，也可以做分类.
2.4.4.1	CART分类树算法
1.最优特征选择方法
CART分类树算法使用基尼系数来选择决策树的特征，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。
在分类问题中，假设有K个类别，第k个类别的概率为pk, 则基尼系数的表达式为：
 
如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：
 

对于个给定的样本D,假设有K个类别, 第k个类别的数量为Ck,则样本D的基尼系数表达式为：
 
特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：

 

而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。

2.对于连续特征改进:
C4.5使用的是信息增益比，而CART分类树使用的是基尼系数将连续的特征离散化。选择基尼系数最小的点作为该连续特征的二元离散分类点,其他和C4.5一样的.
与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
3.	离散特征处理
对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。
ID3或者C4.5:
如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点,这样导致决策树是多叉树。
CART分类树:
不停的二分离散特征,CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面会在子节点继续选择到特征A来划分A1和A3。
在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。C4.5处理连续属性的时候, 如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
4. 算法的具体流程
算法输入是训练集D，基尼系数的阈值，样本个数阈值。
输出是决策树T。
我们的算法从根节点开始，用训练集递归的建立CART树。
1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。
2) 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。
3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，缺失值的处理方法和上篇的C4.5算法里描述的相同。
4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.
5) 对左右的子节点递归的调用1-4步，生成决策树。
2.4.4.2	CART回归树算法
CART回归树和CART分类树的建立算法大部分是类似的， 两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。是连续值，那么那么这是一颗回归树。
除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
1)连续值的处理方法不同
2)决策树建立后做预测的方式不同。
 对于连续值:
CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：
 
其中，c1为D1数据集的样本输出均值，c2为D2数据集的样本输出均值。
对于决策树建立后做预测的方式，
CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。
回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。
2.4.4.3 CART树算法的剪枝
由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题

(1)	CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。
(2)	随机森林 --构建大量的决策树组成森林来防止过拟合；虽然单个树可能存在过拟合，但通过广度的增加就会消除过拟合现象

CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样.
CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝后的预测能力，选择泛化能力最好的剪枝策略作为最终的CART树。
 
1.剪枝的损失函数度量:
在剪枝的过程中，对于任意的一子树T,其损失函数为：
　　　 
其中，α为正则化参数，这和线性回归的正则化一样。C(Tt)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|Tt|是子树T的叶子节点的数量。

当α=0时，即没有正则化，原始的生成的CART树即为最优子树。
当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。
。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。
对于固定的α，一定存在使损失函数Cα(T)最小的唯一子树。
 
剪枝的思路，对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是
 

Tt和T有相同的损失函数，但是T节点更少，因此可以对子树Tt进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。
CART树的交叉验证策略
可以计算出每个子树是否剪枝的阈值α，如果把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。
CART树的剪枝算法。

输入是CART树建立算法得到的原始决策树T。
输出是最优决策子树Tα
1）初始化αmin=∞， 最优子树集合ω={T}。
2）从叶子节点开始自下而上计算各内部节点t的训练误差损失函数Cα(Tt)（回归树为均方差，分类树为基尼系数）, 叶子节点数|Tt|，以及正则化阈值
 
3) 得到所有节点的α值的集合M。
4）从M中选择最大的值αk，自上而下的访问子树t的内部节点，如果
 
进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到αk对应的最优子树Tk
5）最优子树集合ω=ω∪Tk， M=M−{αk}。
6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω
7) 采用交叉验证在ω选择最优子树Tα
 
首先我们看看决策树算法的优点：
　　　　1）简单直观，生成的决策树很直观。
　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。
　　　　3）使用决策树预测的代价是O(log2m)O(log2m)。 m为样本数。
　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
　　　　5）可以处理多维度输出的分类问题。
　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释
　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。
　　　　8） 对于异常点的容错能力好，健壮性高。
　　　　我们再看看决策树算法的缺点:
　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。
　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。
2.4.5  scikit-learn决策树算法类库 
scikit-learn决策树算法类库内部实现是使用了调优过的CART树算法，既可以做分类，又可以做回归。
分类决策树的类对应的是DecisionTreeClassifier
回归决策树的类对应的是DecisionTreeRegressor
两者的参数定义几乎完全相同，但是意义不全相同

 
 
 
 
 
 
 
 
 

 
 
2.4.6  随机森林
随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。
决策树经常会过拟合
解决办法: 随机从数据中挑选几列,并根据这些列构建决策树,然后随机选取其他几列,再次构建决策树,然后让决策树进行选择,当我们有新的数据的时候 ,只需要让所有的决策树做出预测,并选取结果中显示最多的,由于利用随机的特征列构建的多个决策树做出了预测,这种方法称为随机森林. 
超参数
1)最大深度
决策树的最大深度指树根和叶子之间的最大长度。
当决策树的最大深度为 k 时，它最多可以拥有 2^k 片叶子。
2)每片叶子的最小样本数
在分裂节点时，很有可能一片叶子上有 99 个样本，而另一片叶子上只有 1 个样本。这将使我们陷入困境，并造成资源和时间的浪费。如果想避免这种问题，我们可以设置每片叶子允许的最小样本数。
这个数字可以被指定为一个整数，也可以是一个浮点数。如果它是整数，它将表示这片叶子上的最小样本数。如果它是个浮点数，它将被视作每片叶子上的最小样本比例。比如，0.1 或 10% 表示如果一片叶子上的样本数量小于该节点中样本数量的 10%，这种分裂将不被允许。
3)每次分裂的最小样本数
这个参数与每片叶子上的最小样本树相同，只不过是应用在节点的分裂当中。
4)最大特征数
有时，我们会遇到特征数量过于庞大，而无法建立决策树的情况。在这种状况下，对于每一个分裂，我们都需要检查整个数据集中的每一个特征。这种过程极为繁琐。而解决方案之一是限制每个分裂中查找的特征数。如果这个数字足够庞大，我们很有可能在查找的特征中找到良好特征（尽管也许并不是完美特征）。然而，如果这个数字小于特征数，这将极大加快我们的计算速度。
较小的最大深度 可能导致欠拟合
较大的最大深度 可能导致过拟合
较小的每片叶子的最小样本数 可能导致过拟合
较大的每片叶子的最小样本数 可能导致欠拟合
2.5	K近邻法(KNN)
2.5.1  KNN算法流程
KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别
KNN方法既可以做分类，也可以做回归.
KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。
KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。
KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。
KNN算法三个要素:k值的选取、距离度量的方式、分类决策规则。
分类决策规则：一般是选择多数表决法
k值的选择：没有固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。
（1）较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
（2）较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。
距离的度量：最常用的是欧式距离，即对于两个n维向量x和y，两者的欧式距离定义为：
 
曼哈顿距离，定义为：
 
闵可夫斯基距离(Minkowski Distance)，定义为：
 

就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类.
KNN算法的描述为：
1）计算测试数据与各个训练数据之间的距离；
2）按照距离的递增关系进行排序；
3）选取距离最小的K个点；
4）确定前K个点所在类别的出现频率；
5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

2.5.2  Scikit-learn中的KNN
在scikit-learn 中，与近邻法这一大类相关的类库都在sklearn.neighbors包之中。
KNeighborsClassifier：KNN分类树的类
KNeighborsRegressor：KNN回归树的类
RadiusNeighborsClassifier：限定半径最近邻分类树的类
RadiusNeighborsRegressor：限定半径最近邻回归树的类
NearestCentroid：最近质心分类算法

其中，KNN分类和回归的类参数完全一样。
限定半径最近邻法分类和回归的类的主要参数也和KNN基本一样。
最近质心分类算法，由于它是直接选择最近质心来分类，所以仅有两个参数，距离度量和特征选择距离阈值
参数小结：
 
 
 
 

 
 





2.6	支持向量机（SVM）
SVM是一个二元分类算法，线性分类和非线性分类都支持。也支持多元分类，同时经过扩展，也能应用于回归问题。
支持向量机（SVM）的基本模型是在特征空间上找到最佳的分离超平面(将分类的决策边界统称为超平面)使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。可以把数据集从低维映射到高维，使得原来线性不可分的数据集变得线性可分。
好的分类边界（也就是求的超平面）要距离最近的训练点越远越好，因为这样可以减低分类器的泛化误差。
支持向量机 是一种 二分类模型,不同于 感知机 是因为 SVM学习策略是间隔最大化，可以将该问题理解为凸二次规划问题，也可以将该问题理解为正则化的合叶损失函数最小化问题，我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为间隔margin. 我们希望这个margin尽可能的大。
2.6.1	 SVM 线性分类模型
2.6.1.1 感知机的分类原理
感知机的模型就是尝试找到一条直线，能够把二元数据隔离开。
放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。对于这个分离的超平面，定义为 ：
 

在超平面wTx+b=0 上方的定义为y=1 ,在超平面下方的定义为y=−1 
满足这个条件的超平面并不止一个
这么多的可以分类的超平面，哪个是最好的呢？或者说哪个是泛化能力最强的呢?
2.6.1.2 感知机模型的损失函数优化
思想是让所有误分类的点(定义为M)到超平面的距离和最小，即最小化下式：
 

在感知机模型中，我们采用的是保留分子，固定分母||w||2=1,即最终感知机模型的损失函数为：

 
2.6.1.3 函数间隔与几何间隔
在分离超平面固定为wTx+b=0的时候，|wTx+b|表示点x到超平面的相对距离。通过观察wTx+b和y是否同号，判断分类是否正确
函数间隔并不能正常反应点到超平面的距离
几何间隔γ,定义为：

 

几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。
2.6.1.4 支持向量
在感知机模型中，让离超平面比较近的点尽可能的远离超平面，分类效果会好有一些。 
和超平面平行的保持一定的函数距离的这两个超平面对应的向量，我们定义为支持向量，如下图虚线所示。
 
支持向量到超平面的距离为
 
两个支持向量之间的距离为
 
2.6.1.5  svm模型目标函数与优化
SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：
 
一般取函数间隔γ′为1，这样我们的优化函数定义为：
 
也就是说，我们要在约束条件
 
感知机是固定分母优化分子，而SVM是固定分子优化分母，同时加上了支持向量的限制。
 
的最大化，等同于
  
的最小化。
这样SVM的优化函数等价于：
 
通过拉格朗日函数将优化目标转化为无约束的优化函数为：
 
由于引入了朗格朗日乘子，优化目标变成：
 
这个优化函数满足KKT条件，也就是说，可以通过拉格朗日对偶将优化问题转化为等价的对偶问题来求解
 
可以先求优化函数对于w和b极小值。接着再求拉格朗日乘子α的极大值。
 
从上两式子可以看出，已经求得了w和α关系，只要后面接着能够求出优化函数极大化对应的α，就可以求出我们的w了，至于b，由于上两式已经没有b，所以最后的b可以有多个。
可以带入优化函数L(w,b,α)消去w了。定义:
 
将w替换为α的表达式以后的优化函数ψ(α)的表达式：

 
通过对w,b极小化以后，优化函数ψ(α)仅仅只有α向量做参数。
只要能够极大化ψ(α)，就可以求出此时对应的α，进而求出w,b
对ψ(α)求极大化的数学表达式如下:
 
可以去掉负号，即为等价的极小化问题如下：
 
假设通过SMO算法，得到了对应的α的值α*。
 
 
假设有S个支持向量，则对应我们求出S个b*,理论上这些b*都可以作为最终的结果， 但是一般采用一种更健壮的办法，即求出所有支持向量所对应的b*，然后将其平均值作为最后的结果。
注意到对于严格线性可分的SVM，b的值是有唯一解的，也就是这里求出的所有b*都是一样的
怎么得到支持向量呢？根据KKT条件中的对偶互补条件:
 

如果αi>0则有
 
即点在支持向量上,否则如果αi=0
则有
 
即样本在支持向量上或者已经被正确分类。

2.6.1.6  线性可分svm 算法流程
输入是线性可分的m个样本(x1,y1),(x2,y2),...,(xm,ym),其中x为n维特征向量。y为二元输出，值为1，或者-1.

输出是分离超平面的参数w*和b*和分类决策函数。

算法过程如下：

1）	构造约束优化问题
 
2）	用SMO算法求出上式最小时对应的α向量的值α*向量.
 
 
线性可分SVM的学习方法对于非线性的数据集是没有办法使用的， 有时候不能线性可分的原因是线性数据集里面多了少量的异常点，由于这些异常点导致了数据集不能线性可分， 那么怎么可以处理这些异常点使数据集依然可以用线性可分的思想呢？ 
2.6.1.7 线性分类SVM的软间隔最大化
所谓的软间隔，是相对于硬间隔说的，上一节线性分类SVM的学习方法属于硬间隔最大化。
硬间隔最大化的条件：
 
SVM对训练集里面的每个样本(xi,yi)引入了一个松弛变量ξi≥0,使函数间隔加上松弛变量大于等于1，也就是说：
 

对比硬间隔最大化，可以看到对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，松弛变量不能白加，这是有成本的，每一个松弛变量ξi, 对应了一个代价ξi，这个就得到了软间隔最大化的SVM学习条件如下：
 
这里,C>0为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。C越大，对误分类的惩罚越大，C越小，对误分类的惩罚越小。
希望尽量 小，误分类的点尽可能的少。C是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。
线性分类SVM的软间隔最大化目标函数的优化
首先将软间隔最大化的约束问题用拉格朗日函数转化为无约束问题如下：　　
 
要优化的目标函数是：

 
先求优化函数对于w,b,ξ的极小值, 接着再求拉格朗日乘子α和 μ的极大值。
首先求优化函数对于w,b,ξ的极小值，这个可以通过求偏导数求得：
 
利用上面的三个式子去消除w和b了,最后得到:
 
这个式子和线性可分SVM的一样。唯一不一样的是约束条件。现在看看优化目标的数学形式：
 
 
 
硬间隔是αi>=0,软间隔约束条件是0≤αi≤C。依然可以通过SMO算法来求上式极小化时对应的α 向量就可以求出w和b 了。
软间隔最大化时的支持向量
 
 
 
 
软间隔最大化的线性可分SVM的算法过程
输入是线性可分的m个样本(x1,y1),(x2,y2),...,(xm,ym),其中x为n维特征向量。y为二元输出，值为1，或者-1.
输出是分离超平面的参数w*和b*分类决策函数。
 
2.6.2 线性不可分SVM和核函数的原理
遇到线性不可分的样例时，常用做法是把样例特征映射到高维空间中去(如上一节的多项式回归）但是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到令人恐怖的。
此时，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数好在它在低维上进行计算，而将实质上的分类效果（利用了内积）表现在了高维上，这样避免了直接在高维空间中的复杂计算，真正解决了SVM线性不可分的问题。
一般说的核函数都是正定核函数，一个函数要想成为正定核函数，必须满足他里面任何点的集合形成的Gram矩阵是半正定的。
 

scikit-learn中默认可选的就是下面几个核函数。
线性核函数
线性核函数（Linear Kernel）其实就是线性可分SVM，表达式为：
 
就是说，线性可分SVM可以和线性不可分SVM归为一类，区别仅仅在于线性可分SVM用的是线性核函数。
多项式核函数
多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：
 
其中，γ,r,d都需要自己调参定义。
高斯核函数
高斯核函数，在SVM中也称为径向基核函数（RBF），它是非线性分类SVM最主流的核函数。libsvm默认的核函数就是它。表达式为：
 

Sigmoid核函数
Sigmoid核函数也是线性不可分SVM常用的核函数之一，表达式为：
 
分类SVM的算法小结
输入是m个样本(x1,y1),(x2,y2),...,(xm,ym),其中x为n维特征向量。y为二元输出，值为1，或者-1.
输出是分离超平面的参数w*和b*和分类决策函数。
过程：
 
 

 
SMO算法每次只优化两个变量，将其他的变量都视为常数。由于
假如将α3,α4,...,αm固定，那么α1,α2之间的关系也确定了。这样SMO算法将一个复杂的优化算法转化为一个比较简单的两变量优化问题。

2.6.3  svm 回归模型
SVM也可以用于回归模型
回归模型，优化目标函数可以继续和SVM分类模型保持一致：
 
约束条件呢？对于回归模型，目标是让训练集中的每个点(xi,yi),尽量拟合到一个线性模型yi =w∙ϕ(xi)+b。
对于一般的回归模型，用均方差作为损失函数,但是SVM不是这样定义损失函数的。
SVM需要定义一个常量ϵ>0,对于某一个点(xi,yi)
如果|yi−w∙ϕ(xi)−b|≤ϵ，则完全没有损失
如果|yi−w∙ϕ(xi)−b|>ϵ,则对应的损失为|yi−w∙ϕ(xi)−b|−ϵ，
这个均方差损失函数不同，如果是均方差，那么只要yi−w∙ϕ(xi)−b≠0，那么就会有损失。
 
如下图所示，在蓝色条带里面的点都是没有损失的，但是外面的点的是有损失的，损失大小为红色线的长度。
 
SVM回归模型的目标函数的原始形式
目标函数如下：
 
 
 
 SVM回归模型的目标函数的对偶形式
 
 
2.6.4  SVM算法特点
SVM算法的主要优点有：
　1) 解决高维特征的分类问题和回归问题很有效,在特征维度大于样本数时依然有很好的效果。
　2) 仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。
3) 有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。
　4)样本量不是海量数据的时候，分类准确率高，泛化能力强。
SVM算法的主要缺点有：
　1) 如果特征维度远远大于样本数，则SVM表现一般。
　2) SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。
　3）非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。
　4）SVM对缺失数据敏感。

2.6.4  SVM 在scikit-learn 应用
scikit-learn中SVM的算法库分为两类：
一类是分类的算法库，包括SVC， NuSVC，和LinearSVC 3个类。
一类是回归算法库，包括SVR， NuSVR，和LinearSVR 3个类。
相关的类都包裹在sklearn.svm模块之中。



对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用。
　对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数。
如果有经验知道数据是线性可以拟合的，那么使用LinearSVC去分类 或者LinearSVR去回归，它们不需要我们去慢慢的调参去选择各种核函数以及对应参数，速度也快。
如果对数据分布没有什么经验，一般使用SVC去分类或者SVR去回归，这就需要我们选择核函数以及对核函数调参了。
对训练集训练的错误率或者说支持向量的百分比有要求的时候，可以选择NuSVC分类 和 NuSVR 。它们有一个参数来控制这个百分比。
SVM分类算法和回归算法
对于SVM分类算法，其原始形式是：
 
 
通过拉格朗日函数以及对偶化后的形式为：
 
其中和原始形式不同的α为拉格朗日系数向量。K(xi,xj)为我们要使用的核函数



对于SVM回归算法，其原始形式是：
 
 
通过拉格朗日函数以及对偶化后的形式为：
 
SVM核函数概述
在scikit-learn中，内置的核函数一共有4种，
　1）线性核函数（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积，LinearSVC 和 LinearSVR 只能使用它。
　2)  多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)d，其中，γ,r,d都需要自己调参定义,比较麻烦。
　3）高斯核函数（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是libsvm默认的核函数，当然也是scikit-learn默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||2)， 其中，γ大于0，需要自己调参定义。
　4）Sigmoid核函数（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ,r都需要自己调参定义。
　注意：一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。　　
 调参小结：
1）一般推荐在做训练之前对数据进行归一化，当然测试集中的数据也需要归一化。
2）在特征数非常多的情况下，或者样本数远小于特征数的时候，使用线性核，效果已经很好，并且只需要选择惩罚系数C即可。
3）在选择核函数时，如果线性拟合不好，一般推荐使用默认的高斯核'rbf'。这时主要需要对惩罚系数C和核函数参数γ进行艰苦的调参，通过多轮的交叉验证选择合适的惩罚系数C和核函数参数γ。
4）理论上高斯核不会比线性核差，但是这个理论却建立在要花费更多的时间来调参上。所以实际上能用线性核解决问题尽量使用线性核。
分类参数：
 
 
 
 
 
 
 
 
 
 
 
回归参数：
 
 
 
 
 
SVM RBF 主要超参数概述　
1.	SVM分类模型
这两个超参数分别是惩罚系数C和RBF核函数的系数γ。当然如果是nu-SVC的话，惩罚系数C代替为分类错误率上限nu。
（1）惩罚系数C
即松弛变量的系数。它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数。
当C比较大时，损失函数也会越大，这意味着不愿意放弃比较远的离群点。这样会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合。
当C比较小时，意味不理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单。scikit-learn中默认值是1。
（2）RBF核函数的参数γ
RBF 核函数K(x,z)=exp(−γ||x−z||2)，γ>0，γ主要定义了单个样本对整个分类超平面的影响
当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多。
scikit-learn中默认值是:   1/样本特征数
（3）综合C和γ一起
当C比较大， γ比较大时，会有更多的支持向量，模型会比较复杂，容易过拟合一些。
如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少。
2.	SVM回归模型
SVM回归模型的RBF核，除了惩罚系数C和RBF核函数的系数γ之外，还多了一个损失距离度量ϵ。如果是nu-SVR的话，损失距离度量ϵ代替为分类错误率上限nu。 
对于惩罚系数C和RBF核函数的系数γ，回归模型和分类模型的作用基本相同。（1）损失距离度量ϵ
它决定了样本点到超平面的距离损失
当ϵ比较大时，损失|yi−w∙ϕ(xi)−b|−ϵ较小，更多的点在损失距离范围之内，而没有损失,模型较简单
当ϵ比较小时，损失函数会较大，模型也会变得复杂。scikit-learn中默认值是0.1。
（2）综合C、γ、e一起
如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看
当C比较大， γ比较大，ϵ比较小时，会有更多的支持向量，模型会比较复杂，容易过拟合一些。
如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少。
3.	SVM RBF 主要调参方法
对于SVM的RBF核，主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类。
当然也可以使用cross_val_score类来调参，但是个人觉得没有GridSearchCV方便。
 　将GridSearchCV类用于SVM RBF调参时要注意的参数有：
　1) estimator :即我们的模型，此处就是带高斯核的SVC或者SVR
　2) param_grid：即我们要调参的参数列表。 比如用SVC分类模型的话，那么param_grid可以定义为{"C":[0.1, 1, 10], "gamma": [0.1, 0.2, 0.3]}，这样就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数。
　3) cv: S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3,如果样本较多的话，可以适度增大cv的值。
　网格搜索结束后，可以得到最好的模型estimator, param_grid中最好的参数组合，最好的模型分数。
2.7  集成方法
集成学习不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等。 
集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。
第一种就是所有的个体学习器都是一个种类的，或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。
第二种是所有的个体学习器不全是一个种类的，或者说是异质的。比如有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。
目前来说，同质个体学习器的应用是最广泛的，一般常说的集成学习的方法都是指的同质个体学习器。使用最多的模型是CART决策树和神经网络。
同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。 
2.7.1集成学习之boosting
 
Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。
Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。只要是boosting大家族的算法，都要解决这4个问题：
1）如何计算学习误差率e?
2) 如何得到弱学习器权重系数α?
3）如何更新样本权重D?
4) 使用何种结合策略？
2.7.1.1  Adaboost算法的基本思路
1. AdaBoost二元分类问题算法流程
输入为样本集T={(x,y1),(x2,y2),...(xm,ym)}，输出为{-1, +1}，弱分类器算法, 弱分类器迭代次数K。
输出为最终的强分类器f(x)
1) 初始化样本集权重为
 
2) 对于k=1,2，...K:
a) 使用具有权重Dk的样本集来训练数据，得到弱分类器Gk(x)
b) 计算Gk(x)的分类误差率
 
c) 计算弱分类器的系数
                 
如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率小的弱分类器权重系数越大。
 d) 更新样本集的权重分布
 
3) 构建最终分类器为：
 
对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数
 
其中R为类别数。从上式可以看出，如果是二元分类，R=2，则上式和我们的二元分类算法中的弱分类器的系数一致。
2. AdaBoost回归问题算法流程
输入为样本集T={(x,y1),(x2,y2),...(xm,ym)}，弱学习器算法, 弱学习器迭代次数K。
输出为最终的强学习器f(x)
1) 初始化样本集权重为
 
2) 对于k=1,2，...K:
   a) 使用具有权重Dk的样本集来训练数据，得到弱学习器Gk(x)
   b) 计算训练集上的最大误差
 
   c) 计算每个样本的相对误差:
 
    d) 计算回归误差率
 
　e) 计算弱学习器的系数
 
  f) 更新样本集的权重分布为
 
3) 构建最终强学习器为：
 
3.	AdaBoost算法正则化
为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代
 
　ν的取值范围为0<ν≤1。对于同样的训练集学习效果，较小的ν意味着需要更多的弱学习器的迭代次数。通常用步长和迭代最大次数一起来决定算法的拟合效果。
4.	AdaBoost算法优缺点
Adaboost的主要优点有：
1）Adaboost作为分类器时，分类精度很高
2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
3）作为简单的二元分类器时，构造简单，结果可理解。
4）不容易发生过拟合
Adaboost的主要缺点有：
1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。
4. scikit-learn Adaboost类库使用小结
scikit-learn中Adaboost类库是AdaBoostClassifier和AdaBoostRegressor两个， AdaBoostClassifier用于分类，AdaBoostRegressor用于回归。
　AdaBoostClassifier使用了两种Adaboost分类算法的实现，SAMME和SAMME.R。而AdaBoostRegressor则使用了上面的Adaboost回归算法的实现，即Adaboost.R2。
　对Adaboost调参时，主要要对两部分内容进行调参，第一部分是对Adaboost的框架进行调参， 第二部分是对选择的弱分类器进行调参。
（1）AdaBoostClassifier和AdaBoostRegressor框架参数
base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即弱分类学习器或者弱回归学习器。常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果选择的AdaBoostClassifier算法是SAMME.R，则弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。
algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。
loss：这个参数只有AdaBoostRegressor有，Adaboost.R2算法需要用到。有线性‘linear’, 平方‘square’和指数 ‘exponential’三种选择, 默认是线性，一般使用线性就足够了，除非你怀疑这个参数导致拟合程度不好。这个值对应了第k个弱分类器的中第i个样本的误差的处理，即： 
 
Ek为训练集上的最大误差
 
n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，常常将n_estimators和下面介绍的参数learning_rate一起考虑。
learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数ν，加上了正则化项，强学习器的迭代公式为fk(x)=fk−1(x)+ναkGk(x)。ν的取值范围为0<ν≤1。对于同样的训练集拟合效果，较小的ν意味着需要更多的弱学习器的迭代次数。通常用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的ν开始调参，默认是1。
（2）AdaBoostClassifier和AdaBoostRegressor弱学习器参数
默认的决策树弱学习器的参数。
即CART分类树DecisionTreeClassifier和CART回归树DecisionTreeRegressor。
max_features:划分时考虑的最大特征数
默认是"None",意味着划分时考虑所有的特征数；
如果是"log2"意味着划分时最多考虑log2N个特征；
如果是"sqrt"或者"auto"意味着划分时最多考虑√N（对N开根号）个特征。
如果是整数，代表考虑的特征绝对数。
如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
一般来说，如果样本特征数不多，比如小于50，用默认的"None"就可以了，如果特征数非常多，可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
max_depth决策树最大深度
默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。
一般来说，数据少或者特征少的时候可以不管这个值。
如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。
min_samples_split内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件
如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2.
如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
min_samples_leaf:叶子节点最少样本数 这个值限制了叶子节点最少的样本数
如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。
如果样本量不大，不需要管这个值。
如果样本量数量级非常大，则推荐增大这个值。
min_weight_fraction_leaf叶子节点最小的样本权重和，这个值限制了叶子节点所有样本权重和的最小值，
如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。
max_leaf_nodes:最大叶子节点数 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。
2.7.2 集成学习之bagging
 
bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，就可以得到T个采样集，对于这T个采样集，可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。
对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstap sampling）,即对于m个样本的原始训练集，每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。
bagging对于弱学习器没有限制，最常用的一般也是决策树和神经网络。
 
bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。
由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。
1.	bagging算法流程
输入为样本集D={(x,y1),(x2,y2),...(xm,ym)}，弱学习器算法, 弱分类器迭代次数T。
输出为最终的强分类器f(x)
　1）对于t=1,2...,T:
　　　a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
　　　b)用采样集Dt训练第t个弱学习器Gt(x)
　2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。
2.随机森林算法流程
RF使用了CART决策树作为弱学习器
输入为样本集D={(x,y1),(x2,y2),...(xm,ym)}，弱分类器迭代次数T。
输出为最终的强分类器f(x)
1）对于t=1,2...,T:
　　a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
　  b)用采样集Dt训练第t个决策树模型Gt(x)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分（nsub）样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分（nsub 越小，则模型约健壮，此时对于训练集的拟合程度会变差。模型的方差会减小，但是偏倚会增大，实际中，一般会通过交叉验证调参获取一个合适的nsub的值。）
2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。
3. RF的优缺点
RF的主要优点有：
1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。
2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
3） 在训练后，可以给出各个特征对于输出的重要性
4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
6） 对部分特征缺失不敏感。
RF的主要缺点有：
1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
2) 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
4. scikit-learn随机森林调参
在scikit-learn中，RF的分类类是RandomForestClassifier，回归类是RandomForestRegressor。当然RF的变种Extra Trees也有， 分类类ExtraTreesClassifier，回归类ExtraTreesRegressor。由于RF和Extra Trees的区别较小，调参方法基本相同，本文只关注于RF的调参。
RF需要调参的参数也包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。
RF框架参数
n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，计算量会太大，并且n_estimators到一定的数量后，再增大n_estimators获得的模型提升会很小，所以一般选择一个适中的数值。默认是100。
oob_score :即是否采用袋外样本来评估模型的好坏。默认识False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。
criterion: 即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。
从上面可以看出， RF重要的框架参数比较少，主要需要关注的是 n_estimators，即RF最大的决策树个数。
RF决策树参数
max_features:RF划分时考虑的最大特征数，可以使用很多种类型的值，默认是"auto",意味着划分时最多考虑√N个特征；
如果是"log2"意味着划分时最多考虑log2N个特征；
如果是"sqrt"或者"auto"意味着划分时最多考虑√N个特征。
如果是整数，代表考虑的特征绝对数。
如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
一般用默认的"auto"就可以了，如果特征数非常多，可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
max_depth: 决策树最大深度默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。
min_samples_split: 内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
min_samples_leaf: 叶子节点最少样本数这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
min_weight_fraction_leaf叶子节点最小的样本权重和：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意这个值了。
max_leaf_nodes: 最大叶子节点数通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。
min_impurity_split: 节点划分最小不纯度 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。
上面决策树参数中最重要的包括最大特征数max_features， 最大深度max_depth， 内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。
2.7.3集成学习之结合策略
2.7.3.1 平均法
对于数值类的回归预测问题，即对于若干个弱学习器的输出进行平均得到最终的预测输出。
最简单的平均是算术平均，也就是说最终预测是
 
如果每个个体学习器有一个权重w，则最终预测是
 
其中wi是个体学习器hi的权重，通常有
 
2.7.3.2  投票法
对于分类问题的预测，通常使用的是投票法。假设预测类别是{c1,c2,...cK},对于任意一个预测样本x，T个弱学习器的预测结果分别是(h1(x),h2(x)...hT(x))。
最简单的投票法是相对多数投票法，也就是少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
稍微复杂的投票法是绝对多数投票法，也就是常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。
更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

2.7.3.3 学习法
对于学习法，代表方法是stacking，当使用stacking的结合策略时， 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
在这种情况下，将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。
三  深度学习
深度神经网络（DNN），也叫多层感知机
感知机的模型，它是一个有若干输入和一个输出的模型，如下图:
 
输出和输入之间学习到一个线性关系，得到中间输出结果
 
接着是一个神经元激活函数:
 
这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。
神经网络则在感知机的模型上做了扩展，总结下主要有三点：
1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力
2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。
 
3） 对激活函数做扩展，感知机的激活函数是sign(z),虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如在逻辑回归里面使用过的Sigmoid函数，即：
 
还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。
3.1深度神经网络（DNN）
3.1.1 DNN的基本结构
DNN内部的神经网络层可以分为三类，第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。
层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。DNN即一个线性关系z=∑wixi+b加上一个激活函数σ(z)。

3.1.2 DNN前向传播算法
激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN,利用和感知机一样的思路，利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。
 
 
 
 

3.1.3 DNN反向传播算法（BP）
训练样本计算出的输出是是随机选择一系列W,b用前向传播算法计算出来的。
损失函数，DNN可选择的损失函数有不少，有均方差。即对于每个样本，期望最小化下式：
 
由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种， 实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。
3.1.4 算法总结
输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长α,最大迭代次数MAX与停止迭代阈值ϵ，输入的m个训练样本{(x1,y1),(x2,y2),...,(xm,ym)}
输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b
1．初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。
 2. for iter to 1 to MAX：
   （1） for i =1 to m：
a)	将DNN输入a1设置为xi
b)	for l=2 to L，循环总层数，进行前向传播算法计算每层训练样本的输出。
 
c)	通过损失函数计算输出层（第L层）的梯度（损失函数对w,b的偏导）δi,L
 
d)	for l= L-1 to 2, 迭代进行反向传播算法计算各层的梯度
 
（2）for l= 2 to L，更新第l层的Wl,bl:
 
（3）如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环到步骤3。
3. 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。
 
3.1.5 BP举例
 
第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数默认为sigmoid函数。
现在赋上如图初值：
其中，输入数据  i1=0.05，i2=0.10;
　　　输出数据 o1=0.01,o2=0.99;
　　　初始权重  w1=0.15,w2=0.20,w3=0.25,w4=0.30;
　　　　　　　　w5=0.40,w6=0.45,w7=0.50,w8=0.55
目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。
Step 1 前向传播
1.输入层---->隐含层：
计算神经元h1的输入加权和：
 
神经元h1的输出o1:(此处用到激活函数为sigmoid函数)：
 
同理，可计算出神经元h2的输出o2：
       　  
2.隐含层---->输出层：
计算输出层神经元o1和o2的值：
 
 
这样前向传播的过程就结束了，得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，现在对误差进行反向传播，更新权值，重新计算输出。
Step 2 反向传播
1.计算总误差
总误差：(square error)   均方误差----损失函数
 
但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和：
 
 
 
2.隐含层---->输出层的权值更新：
以权重参数w5为例，如果想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）
 
下面的图可以更直观的看清楚误差是怎样反向传播的：
 
现在我们来分别计算每个式子的值：
计算 ：
 
计算 ：
 
（这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下） 
计算 ：
 
最后三者相乘：
 
这样就计算出整体误差E(total)对w5的偏导值。
回过头来再看看上面的公式，我们发现：
 
为了表达方便，用 来表示输出层的误差：
 
因此，整体误差E(total)对w5的偏导公式可以写成：
 
如果输出层误差计为负的话，也可以写成：
 
最后我们来更新w5的值：
 
（其中， 是学习速率，这里我们取0.5）
同理，可更新w6,w7,w8:
 
3.隐含层---->隐含层的权值更新：
方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)---->net(o1)---->w5,但是在隐含层之间的权值更新时，是out(h1)---->net(h1)---->w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。
 
 
计算 ：
 
先计算 ：
 
 
 
 
同理，计算出：
 
两者相加得到总值：
 
再计算 ：
 
再计算 ：
 
最后，三者相乘：
 
为了简化公式，用sigma(h1)表示隐含层单元h1的误差：
 
最后，更新w1的权值：
 
同理，额可更新w2,w3,w4的权值：
 
这样误差反向传播法就完成了，最后再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为[0.015912196,0.984065734](原输入为[0.01,0.99]),证明效果还是不错的。
3.1.6 DNN的损失函数以及激活函数
3.1.6.1 均方差损失函数+Sigmoid激活函数
Sigmoid激活函数的表达式为：
 
从图上可以看出，对于Sigmoid，当z的取值越来越大后，函数曲线变得越来越平缓，意味着此时的导数σ′(z)也越来越小。同样的，当z的取值越来越小时，也有这个问题。仅仅在z取值为0附近时，导数σ′(z)的取值较大。
反向传播算法中，每一层向前递推都要乘以σ′(z),得到梯度变化值。Sigmoid的这个曲线意味着在大多数时候，梯度变化值很小，导致W,b更新到极值的速度较慢，也就是算法收敛速度较慢。
另一种常见的选择是用交叉熵损失函数来代替均方差损失函数。使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度
3.1.6.2 交叉熵损失函数+Sigmoid激活函数
交叉熵损失函数的形式：
 
当使用交叉熵时，输出层δL的梯度情况。
 
可见δl梯度表达式里面已经没有σ′(z)，均方差损失函数时在δL梯度，
 
对比两者在第L层的δL梯度表达式，就可以看出，使用交叉熵，得到的的δl梯度表达式没有了σ′(z)，梯度为预测值和真实值的差距，这样求得的Wl,bl也不包含σ′(z)，因此避免了反向传播收敛速度慢的问题。
通常情况下，如果使用了sigmoid激活函数，交叉熵损失函数肯定比均方差损失函数好用。
3.1.6.3 对数似然损失函数和softmax激活函数进行DNN分类输出


假设有一个三个类别的分类问题，这样DNN输出层应该有三个神经元，假设第一个神经元对应类别一，第二个对应类别二，第三个对应类别三，这样期望的输出应该是(1,0,0)，（0,1,0）和(0,0,1)这三种。即样本真实类别对应的神经元输出应该无限接近或者等于1，而非该样本真实输出对应的神经元的输出应该无限接近或者等于0。
或者说，希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值对于各类别的输出预测，同时为满足概率模型，这若干个概率值之和应该等于1。
DNN分类模型要求是输出层神经元输出的值在0到1之间，同时所有输出值之和为1。只需要对现有的全连接DNN稍作改良，即可用于解决分类问题。在现有的DNN模型中，可以将输出层第i个神经元的激活函数定义为如下形式：
 
只需要将输出层的激活函数从Sigmoid之类的函数转变为上式的激活函数即可。上式这个激活函数就是softmax激活函数。将DNN用于分类问题，在输出层用softmax激活函数也是最常见的了。
　下面这个例子清晰的描述了softmax激活函数在前向传播算法时的使用。假设我们的输出层为三个神经元，而未激活的输出为3,1和-3，我们求出各自的指数表达式为：20,2.7和0.05，我们的归一化因子即为22.75，这样我们就求出了三个类别的概率输出分布为0.88，0.12和0。
 
对于用于分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，即：
 
 
可见损失函数只和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元的梯度导数直接为0。对于真实类别第i类，对应的第j个w-> wLij对应的梯度计算为：
可见，梯度计算也很简洁，也没有训练速度慢的问题。举个例子，假如对于第2类的训练样本，通过前向算法计算的未激活输出为（1,5,3），则得到softmax激活后的概率输出为：(0.015,0.866,0.117)。由于类别是第二类，则反向传播的梯度应该为：(0.015,0.866-1,0.117)。

 
3.1.7 DNN梯度爆炸、梯度消失与ReLU激活函数
在反向传播的算法过程中，由于使用了是矩阵求导的链式法则，有一大串连乘，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，导致梯度消失，而如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，导致梯度爆炸。
对于梯度爆炸，则一般可以通过调整DNN模型中的初始化参数得以解决。
对于无法完美解决的梯度消失问题，一个可能部分解决梯度消失问题的办法是使用ReLU激活函数，ReLU在卷积神经网络CNN中得到了广泛的应用，在CNN中梯度消失似乎不再是问题。表达式为：
 
大于等于0则不变，小于0则激活后为0
DNN常用的激活函数还有：
 
tanh和sigmoid对比主要的特点是它的输出落在了[-1,1],这样输出可以进行标准化。同时tanh的曲线在较大时变得平坦的幅度没有sigmoid那么大，这样求梯度变化值有一些优势。当然，要说tanh一定比sigmoid好倒不一定，还是要具体问题具体分析。
 
 

3.1.8 DNN 正则化
DNN也会遇到过拟合的问题，需要考虑泛化，就对DNN的正则化方法做一个总结。
3.1.8.1  DNN的L1&L2正则化
DNN的L2正则化
DNN的L2正则化通常的做法是只针对与线性系数矩阵W,而不针对偏倚系数b。
假如每个样本的损失函数是均方差损失函数,则所有的m个样本的损失函数为：
 
加上了L2正则化后的损失函数是：
 
如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化的反向传播算法完全一样，区别仅仅在于进行梯度下降法时，W的更新公式。
W的梯度下降更新公式为：
 
类似的L2正则化方法可以用于交叉熵损失函数或者其他的DNN损失函数 
3.1.8.2  DNN通过集成学习的思路正则化
DNN可以用Bagging的思路来正则化。常用的机器学习Bagging算法中，随机森林是最流行的。它通过随机采样构建若干个相互独立的弱决策树学习器，最后采用加权平均法或者投票法决定集成的输出。在DNN中，一样使用Bagging的思路。不过和随机森林不同的是，这里不是若干个决策树，而是若干个DNN的网络。
首先要对原始的m个训练样本进行有放回随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练DNN。即采用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，最后对N个DNN模型的输出用加权平均法或者投票法决定最终输出。
不过用集成学习Bagging的方法有一个问题，就是DNN模型本来就比较复杂，参数很多。现在又变成了N个DNN模型，这样参数又增加了N倍，从而导致训练这样的网络要花更加多的时间和空间。因此一般N的个数不能太多，比如5-10个就可以了。
3.1.8.3  DNN通过dropout 正则化
所谓的Dropout指的是在用前向传播算法和反向传播算法训练DNN模型时，一批数据迭代时，随机的从全连接DNN网络中去掉一部分隐藏层的神经元。
dropout并不意味着这些神经元永远的消失了。在下一批数据迭代前，会把DNN模型恢复成最初的全连接模型，然后再用随机的方法去掉部分隐藏层的神经元，接着去迭代更新W,b。当然，这次用随机的方法去掉部分隐藏层后的残缺DNN网络和上次的残缺DNN网络并不相同。
dropout的方法： 每轮梯度下降迭代时，它需要将训练数据分成若干批，然后分批进行迭代，每批数据迭代时，需要将原始的DNN模型随机去掉部分隐藏层的神经元，用残缺的DNN模型来迭代更新W,b。每批数据迭代更新完毕后，要将残缺的DNN模型恢复成原始的DNN模型。
dropout模型中的W,b是一套，共享的。所有的残缺DNN迭代时，更新的是同一组W,b；而Bagging正则化时每个DNN模型有自己独有的一套W,b参数，相互之间是独立的。当然他们每次使用基于原始数据集得到的分批的数据集来训练模型，这点是类似的。
dropout会将原始数据分批迭代，因此原始数据集最好较大，否则模型可能会欠拟合。
3.1.8.4  DNN通过增强数据集正则化
增强模型泛化能力最好的办法是有更多更多的训练数据，但是在实际应用中，更多的训练数据往往很难得到。
对于DNN擅长的领域，比如图像识别，语音识别等则是有办法的。以图像识别领域为例，对于原始的数据集中的图像，可以将原始图像稍微的平移或者旋转一点点，则得到了一个新的图像。虽然这是一个新的图像，即样本的特征是新的，但是知道对应的特征输出和之前未平移旋转的图像是一样的。
3.2 卷积神经网络（CNN）
CNN广泛的应用于图像识别，当然现在也应用于NLP等其他领域，本文就对CNN的模型结构做一个总结。
3.2.1 CNN 的基本结构
 
输入层：图中是一个图形识别的CNN模型。可以看出最左边的船的图像就是我们的输入层，计算机理解为输入若干个矩阵，这点和DNN基本相同。
卷积层（Convolution Layer）,这个是CNN特有的，卷积层的激活函数使用的是ReLU。ReLU(x)=max(0,x)。对于卷积后的输出，一般会通过ReLU激活函数，将输出的张量中的小于0的位置对应的元素值都变为0。
池化层(Pooling layer)：在卷积层后面，这个也是CNN特有的，池化层没有激活函数。
卷积层+池化层的组合可以在隐藏层出现很多次，上图中出现两次。而实际上这个次数是根据模型的需要而来的。也可以灵活使用使用卷积层+卷积层，或者卷积层+卷积层+池化层的组合，这些在构建模型的时候没有限制。但是最常见的CNN都是若干卷积层+池化层的组合，如上图中的CNN结构。
全连接层（Fully Connected Layer, 简称FC），在若干卷积层+池化层后面是，全连接层其实就是前面的DNN结构，只是输出层使用了Softmax激活函数来做图像识别的分类
从上面CNN的模型描述可以看出，CNN相对于DNN，比较特殊的是卷积层和池化层。
3.2.2 初识卷积
数学中的卷积：
 
CNN中的二维卷积，定义为：
 
W为卷积核，而X则为输入。如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。但是如果X是多维张量，那么W也是一个多维的张量。
3.2.3 CNN中的卷积层
假如是对图像卷积，其实就是对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到。
举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。假设卷积是一次移动一个像素来卷积的，那么首先对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的S00的元素，值为aw+bx+ey+fz。接着将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样得到了输出矩阵S的S01的元素，同样的方法，可以得到输出矩阵S的S02，S10，S11，S12的元素。
 
最终得到卷积输出的矩阵为一个2x3的矩阵S。
3.2.4 CNN中的池化层
所谓的池化，就是对输入张量的各个子矩阵进行压缩。假如是2x2的池化，那么就将子矩阵的每2x2个元素变成一个元素，如果是3x3的池化，那么就将子矩阵的每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。
要想将输入子矩阵的每nxn个元素变成一个元素，那么需要一个池化标准。常见的池化标准有2个，MAX或者是Average。即取对应区域的最大值或者平均值作为池化后的元素值。
下面这个例子采用取最大值的池化方法。同时采用的是2x2的池化。步幅为2。
首先对红色2x2区域进行池化，由于此2x2区域的最大值为6.那么对应的池化输出位置的值为6，由于步幅为2，此时移动到绿色的位置去进行池化，输出的最大值为8.同样的方法，可以得到黄色区域和蓝色区域的输出值。最终，我们的输入4x4的矩阵在池化后变成了2x2的矩阵。进行了压缩。
 
3.2.5  CNN 的前向传播
CNN的结构，包括输入层、若干的卷积层+ReLU激活函数、若干的池化层，DNN全连接层，以及最后的用Softmax激活函数的输出层。
3.2.5.1  CNN输入层前向传播到卷积层

以图像识别为例
如果样本是二维的黑白图片。这样输入层X就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核W就也是矩阵。
如果样本都是RGB的彩色图片，这样输入X就是3个矩阵，即分别对应R，G和B的矩阵，或者说是一个张量。这时和卷积层相连的卷积核W就也是张量，对应的最后一维的维度为3.即每个卷积核都是3个子矩阵组成。
同样的方法，对于3D的彩色图片之类的样本，输入X可以是4维，5维的张量，那么对应的卷积核W也是个高维的张量。
不管维度多高，对于输入，前向传播的过程可以表示为：
 
和DNN的前向传播比较一下，其实形式非常的像，只是这儿是张量的卷积，而不是矩阵的乘法。同时由于W是张量，那么同样的位置，W参数的个数就比DNN多很多了。
都默认输入是3维的张量，即用RBG可以表示的彩色图片。
这里需要自己定义的CNN模型参数是：
1） 一般卷积核不止一个，比如有K个，那么输入层的输出，或者说第二层卷积层的对应的输入就K个。
2） 卷积核中每个子矩阵的的大小，一般都用子矩阵为方阵的卷积核，比如FxF的子矩阵。
3） 填充padding（以下简称P），卷积的时候，为了可以更好的识别边缘，一般都会在输入矩阵在周围加上若干圈的0再进行卷积，加多少圈则P为多少。
4） 步幅stride（以下简称S），即在卷积过程中每次移动的像素距离大小。
3.2.5.2 隐藏层前向传播到卷积层
假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量。这时表达式和输入层的很像，也是
 
其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, σ为激活函数，这里一般都是ReLU。
也可以写成M个子矩阵子矩阵卷积后对应位置相加的形式，即：
 
和上一节唯一的区别仅仅在于，这里的输入是隐藏层来的，而不是输入的原始图片样本形成的矩阵。
需要我们定义的CNN模型参数也和上一节一样，这里需要定义卷积核的个数K，卷积核子矩阵的维度F，填充大小P以及步幅S。
3.2.5.3  隐藏层前向传播到池化层
池化层的处理逻辑是比较简单的，我目的就是对输入的矩阵进行缩小概括。比如输入的若干矩阵是N×N维的，而池化大小是k×k的区域，则输出的矩阵都是 维的。
这里需要需要我们定义的CNN模型参数是：
1）池化区域的大小k
2）池化的标准，一般是MAX或者Average。
3.2.5.4 隐藏层前向传播到全连接层
由于全连接层就是普通的DNN模型结构，因此可以直接使用DNN的前向传播算法逻辑，即：
 
这里的激活函数一般是sigmoid或者tanh。
经过了若干全连接层之后，最后的一层为Softmax输出层。此时输出层和普通的全连接层唯一的区别是，激活函数是softmax函数。
这里需要需要定义的CNN模型参数是：
1）全连接层的激活函数
2）全连接层各层神经元的个数
3.2.5.5  CNN 前向传播算法流程
输入：1个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。
输出：CNN模型的输出aL
1) 根据输入层的填充大小P，填充原始图片的边缘，得到输入张量a1。
2）初始化所有隐藏层的参数W,b　　
3）for l=2 to L−1:
　　a) 如果第l层是卷积层,则输出为
 
    b) 如果第l层是池化层,则输出为：
 
这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。
c) 如果第l层是全连接层,则输出为
 
4)对于输出层第L层:
 
3.2.5.6  CNN 反向传播算法流程
 
 
输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。梯度迭代参数迭代步长α,最大迭代次数MAX与停止迭代阈值ϵ
输出：CNN模型各隐藏层与输出层的W,b
1) 初始化各隐藏层与输出层的各W,b的值为一个随机值
2）for iter to 1 to MAX：
    a) 将CNN输入a1设置为xi对应的张量
    b) for l=2 to L-1，根据下面3种情况进行前向传播算法计算输出：
　b-1) 如果当前是全连接层：则有：
            
  b-2) 如果当前是卷积层：则有：
 
   b-3) 如果当前是池化层：则有
                   
           这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。
c) 计算输出层第L层的输出: 
 
c) 通过损失函数计算输出层的梯度δi,L
d) for l= L-1 to 2, 根据下面3种情况进行进行反向传播算法计算:
d-1)  如果当前是全连接层：
 
d-2) 如果当前是卷积层：
 
Rot180：卷积核要翻转180度
　  d-3) 如果当前是池化层：
 
2-2) for l= 2 to L，根据下面2种情况更新第l层的Wl,bl
2-2-1) 如果当前是全连接层：
     
2-2-2) 如果当前是卷积层，对于每一个卷积核有：
 
2-3) 如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环到步骤3。
3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。

　　在前向传播算法时，池化层一般会用MAX或者Average对输入进行池化，池化的区域大小已知。现在反过来，要从缩小后的误差δl，还原前一次较大区域对应的误差。
　　在反向传播时，首先会把δl的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把δl的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把δl的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做upsample。
 
 
 
3.3 循环神经网络（RNN）
在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。
3.3.1 RNN的模型
 
上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。重点观察右边部分的图。
这幅图描述了在序列索引号t附近RNN的模型。其中：
1）x(t)代表在序列索引号t时训练样本的输入。同样的，x(t−1)和x(t+1)代表在序列索引号t−1和t+1时训练样本的输入。
2）h(t)代表在序列索引号t时模型的隐藏状态。h(t)由x(t)和h(t−1)共同决定。
3）o(t)代表在序列索引号t时模型的输出。o(t)只由模型当前的隐藏状态h(t)决定。
4）L(t)代表在序列索引号t时模型的损失函数。
5）y(t)代表在序列索引号t时训练样本序列的真实输出。
6）U,W,V这三个矩阵是模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。 也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。　　
3.3.2  RNN的前向传播
 
3.3.3 RNN反向传播算法
思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数U,W,V,b,c。
由于是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT。这里所有的U,W,V,b,c在序列的各个位置是共享的，反向传播时更新的是相同的参数。
损失函数这里为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。
对于RNN，由于在序列的每个位置都有损失函数，因此最终的损失L为：
 
其中V,c的梯度计算是比较简单的：
 
W,U,b的梯度计算：
从RNN的模型可以看出，在反向传播时，在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置t+1时的梯度损失两部分共同决定。对于W在某一序列位置t的梯度损失需要反向传播一步步的计算。
定义序列索引t位置的隐藏状态的梯度为：
 
 
Diag 对角矩阵
对于δ(τ)，由于它的后面没有其他的序列索引了，因此有：
 
W,U,b的梯度计算表达式：
 
RNN虽然理论上可以很漂亮的解决序列数据的训练，但是它也像DNN一样有梯度消失时的问题，当序列很长的时候问题尤其严重。因此，上面的RNN模型一般不能直接用于应用领域。在语音识别，手写书别以及机器翻译等NLP领域实际应用比较广泛的是基于RNN模型的一个特例LSTM
3.4  LSTM算法
RNN的特例LSTM（Long Short-Term Memory）避免常规RNN的梯度消失
在RNN模型里，每个序列索引位置t都有一个隐藏状态h(t)。
 
略去每层都有的o(t),L(t),y(t)，则RNN的模型可以简化成如下图的形式：
 
图中可以很清晰看出在隐藏状态h(t)由x(t)和h(t−1)得到。得到h(t)后一方面用于当前层的模型损失计算，另一方面用于计算下一层的h(t+1)。
由于RNN梯度消失的问题，对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是LSTM。由于LSTM有很多的变种，以最常见的LSTM为例讲述。LSTM的结构如下图：
 
从上图中可以看出，在每个序列索引位置t时刻向前传播的除了和RNN一样的隐藏状态h(t)，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态一般称为细胞状态(Cell State)，记为C(t)。如下图所示：
 
除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。LSTM在在每个序列索引位置t的门一般包括遗忘门，输入门和输出门三种。
3.4.1 LSTM 遗忘门
遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：
 
　图中输入的有上一序列的隐藏状态h(t−1)和本序列数据x(t)，通过一个激活函数，一般是sigmoid，得到遗忘门的输出f(t)。由于sigmoid的输出f(t)在[0,1]之间，因此这里的输出f^{(t)}代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为：
 
 
3.4.2 LSTM 输出门
 
输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：
从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为i(t),第二部分使用了tanh激活函数，输出为a(t), 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：
 

3.4.3 LSTM 细胞状态更新
前面的遗忘门和输入门的结果都会作用于细胞状态C(t)。从细胞状态C(t−1)如何得到C(t)。如下图所示：
 
细胞状态C(t)由两部分组成，第一部分是C(t−1)和遗忘门输出f(t)的乘积，第二部分是输入门的i(t)和a(t)的乘积，即：
 
　　　　其中，⊙为Hadamard积，在DNN中也用到过。
3.4.4 LSTM 输出门
输出门，子结构如下：
 
从图中可以看出，隐藏状态h(t)的更新由两部分组成，第一部分是o(t), 它由上一序列的隐藏状态h(t−1)和本序列数据x(t)，以及激活函数sigmoid得到，第二部分由隐藏状态C(t)和tanh激活函数组成, 即：
 
3.4.5 LSTM 前向传播算法
LSTM模型有两个隐藏状态h(t),C(t)，模型参数几乎是： 
 
前向传播过程在每个序列索引位置的过程为：
 
3.4.6 LSTM 反向传播算法
LSTM和RNN的反向传播算法思路一致，也是通过梯度下降法迭代更新所有的参数，关键点在于计算所有参数基于损失函数的偏导数。
在RNN中，为了反向传播误差，通过隐藏状态h(t)的梯度δ(t)一步步向前传播。在LSTM这里也类似。只不过这里有两个隐藏状态h(t)和C(t)。
  
反向传播时只使用了δ(t)C,变量δ(t)h仅在某一层计算用，并没有参与反向传播，这里要注意。如下图所示：
 
 
 
 
3.5 总结
深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。深度学习里的第三类神经网络模型：玻尔兹曼机。


 














四  非监督学习
4.1  K均值聚类
4.1.1  K-Means算法
K-Means算法是无监督的聚类算法。 
 
　上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。
　当然在实际K-Mean算法中，我们一般会多次运行图c和图d，才能达到最终的比较优的类别。
K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。
输入:样本集合D={x1,x2,…,xm}，聚类簇数:k，最大迭代数N
输出：是簇划分C={C1，C2，…,Ck}
过程:
1.从D中随机选取k个样本作为初始均值向量{u1,u2,…,uk},作为k个簇的各自的中心
2. 对于n=1,2,...,N
1)	将簇划分C初始化为Ct=∅ ， t=1,2...k
2)	对于D中的每个样本，For j=1,2,…m
 do
计算样本Xj与各均值向量Ui(1<=i<=k)的距离
 
根据距离最近的均值向量确定xj的簇标记,划入相应的簇
3)	对于每一个簇,for i=1,2…,k 
do
计算新的均值向量Ui’
If Ui’!= Ui:
将当前均值向量ui更新为Ui’,进入2）中,直到Ui’=Ui相等
Else
3. 保持当前均值向量不变,算法停止,得到最终的簇划分

K-Means算法的一些要点。
1）对于K-Means算法，首先要注意的是k值的选择，一般来说，会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。
2）在确定了k的个数后，需要选择k个初始化的质心，就像上图b中的随机质心。由于是启发式方法，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心，最好这些质心不能太近。
4.1.2 K-Means特点
KNN和K-Means 比较：
K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。
当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。
K-Means是个简单实用的聚类算法，这里对K-Means的优缺点做一个总结。　K-Means的主要优点有：
1）原理比较简单，实现也是很容易，收敛速度快。
2）聚类效果较优。
3）算法的可解释度比较强。
4）主要需要调参的参数仅仅是簇数k。
K-Means的主要缺点有：
1）K值的选取不好把握
2）对于不是凸的数据集比较难收敛
3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
4） 采用迭代方法，得到的结果只是局部最优。
5） 对噪音和异常点比较的敏感。
4.1.3 K-Means 在sklearn
在scikit-learn中，包括两个K-Means的算法，
一个是传统的K-Means算法，对应的类是KMeans。
一个是基于采样的Mini Batch K-Means算法，对应的类是MiniBatchKMeans。一般来说，使用K-Means的算法调参是比较简单的。
用KMeans类的话，一般要注意的仅仅就是k值的选择，即参数n_clusters；如果是用MiniBatchKMeans的话，也仅仅多了需要注意调参的参数batch_size，即Mini Batch的大小。

　当然KMeans类和MiniBatchKMeans类可以选择的参数还有不少，但是大多不需要怎么去调参。
KMeans类的主要参数有：
　1) n_clusters: 即k值，一般需要多试一些值以获得较好的聚类效果。
　2）max_iter： 最大的迭代次数，一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。
　3）n_init：用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。
　4）init： 即初始值选择的方式，可以为完全随机选择'random',优化过的'k-means++'或者自己指定初始化的k个质心。一般建议使用默认的'k-means++'。
k-means++：
 
　5）algorithm：有“auto”, “full” or “elkan”三种选择。"full"就是传统的K-Means算法， “elkan”是elkan K-Means算法，对距离的计算方式进行了优化。默认的"auto"则会根据数据值是否是稀疏的，来决定如何选择"full"和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是"full"。一般来说建议直接用默认的"auto"
 
MiniBatchKMeans类的主要参数比KMeans类稍多，主要有：
1)n_clusters: 即k值，和KMeans类的n_clusters意义一样。
2）max_iter：最大的迭代次数，和KMeans类的max_iter意义一样。
3）n_init：用不同的初始化质心运行算法的次数。这里和KMeans类意义稍有不同，KMeans类里的n_init是用同样的训练集数据来跑不同的初始化质心从而运行算法。而MiniBatchKMeans类的n_init则是每次用不一样的采样数据集来跑不同的初始化质心运行算法。
4）batch_size：即用来跑Mini Batch KMeans算法的采样集的大小，默认是100.如果发现数据集的类别较多或者噪音点较多，需要增加这个值以达到较好的聚类效果。
5）init： 即初始值选择的方式，和KMeans类的init意义一样。
6）init_size: 用来做质心初始值候选的样本个数，默认是batch_size的3倍，一般用默认值就可以了。
7）reassignment_ratio: 某个类别质心被重新赋值的最大次数比例，这个和max_iter一样是为了控制算法运行时间的。这个比例是占样本总数的比例，乘以样本总数就得到了每个类别质心可以重新赋值的次数。如果取值较高的话算法收敛时间可能会增加，尤其是那些暂时拥有样本数较少的质心。默认是0.01。如果数据量不是超大的话，比如1w以下，建议使用默认值。如果数据量超过1w，类别又比较多，可能需要适当减少这个比例值。具体要根据训练集来决定。
8）max_no_improvement：即连续多少个Mini Batch没有改善聚类效果的话，就停止算法， 和reassignment_ratio， max_iter一样是为了控制算法运行时间的。默认是10.一般用默认值就足够了。

4.1.4 K值的评估标准
　不像监督学习的分类问题和回归问题，无监督聚类没有样本输出，也就没有比较直接的聚类评估方法。但是可以从簇内的稠密程度和簇间的离散程度来评估聚类的效果。
常见的方法有轮廓系数有：
Silhouette Coefficient和Calinski-Harabasz Index。
轮廓系数适用于实际类别信息未知的情况。
对于单个样本，设a是与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离，轮廓系数为：
 
对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。
轮廓系数取值范围是[−1,1]，同类别样本距离越相近且不同类别样本距离越远，分数越高
	判断：
轮廓系数范围在[-1,1]之间。该值越大，越合理。
s接近1，则说明样本聚类合理；
s接近-1，则说明样本更应该分类到另外的簇；
若s 近似为0，则说明样本在两个簇的边界上。
所有样本的s 的均值称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。 
使用轮廓系数(silhouette coefficient)来确定，选择使系数较大所对应的k值 
sklearn.metrics.silhouette_score sklearn中有对应的求轮廓系数的API
Calinski-Harabasz Index：
Calinski-Harabasz分数值越大则聚类效果越好。
也就是说，类别内部数据的协方差越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz分数会高。
在scikit-learn中，
 Calinski-Harabasz Index对应的方法是metrics.calinski_harabaz_score.  
4.2  层次聚类
单连接：AGNES—将数据集合中的每个样本均看作是一个初始聚类簇,然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并,不断重复直至达到预设的聚类个数 ，可以从下而上地把小的cluster合并聚集。
全连接：DIANA先将所有样本当作一整个簇，然后找出簇中距离最远的两个簇进行分裂，不断重复到预期簇或者其他终止条件。上而下地将大的cluster进行分割。 
从下而上地把小的cluster合并聚集：
 
 层次聚类的缺点是计算量比较大，因为要每次都要计算多个cluster内所有数据点的两两距离。另外，由 于层次聚类使用的是贪心算法，得到的显然只是局域最优，不一定就是全局最优。
4.2.1 scikit-learn之BIRCH类
from sklearn.cluster import Birch
1) threshold:即叶节点每个CF的最大样本半径阈值T，它决定了每个CF里所有样本形成的超球体的半径阈值。一般来说threshold越小，则CF Tree的建立阶段的规模会越大，即BIRCH算法第一阶段所花的时间和内存会越多。但是选择多大以达到聚类效果则需要通过调参决定。默认值是0.5.如果样本的方差较大，则一般需要增大这个默认值。
2) branching_factor：即CF Tree内部节点的最大CF数B，以及叶子节点的最大CF数L。这里scikit-learn对这两个参数进行了统一取值。也就是说，branching_factor决定了CF Tree里所有节点的最大CF数。默认是50。如果样本量非常大，比如大于10万，则一般需要增大这个默认值。选择多大的branching_factor以达到聚类效果则需要通过和threshold一起调参决定
3）n_clusters：即类别数K，在BIRCH算法是可选的，如果类别数非常多，也没有先验知识，则一般输入None，此时BIRCH算法第4阶段不会运行。但是如果有类别的先验知识，则推荐输入这个可选的类别值。默认是3，即最终聚为3类。
4）compute_labels：布尔值，表示是否标示类别输出，默认是True。一般使用默认值挺好，这样可以看到聚类效果。
    在评估各个参数组合的聚类效果时，还是推荐使用Calinski-Harabasz Index，Calinski-Harabasz Index在scikit-learn中对应的方法是metrics.calinski_harabaz_score
4.2  密度聚类
DBSCAN具有噪声的基于密度的聚类方法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。 
DBSCAN 输入接收两个参数  邻域半径值 和点的最小数量
DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数(ϵ, MinPts)用来描述邻域的样本分布紧密程度。其中，ϵ描述了某一样本的邻域距离阈值，MinPts描述了某一样本的距离为ϵ的邻域中样本个数的阈值。
4.2.1优点
（1）聚类速度快且能够有效处理数据集合中的噪声和离群值和发现任意形状的空间聚类；相对的，K-Means之类的聚类算法一般只适用于凸数据集。
（2）与K-MEANS比较起来，不需要输入要划分的聚类个数；不需要指明类的数量 DBSCAN可以根据点的分布密度以及我们输入的邻域以及点的最小量 从而找到类
（3）聚类簇的形状没有偏倚, 相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。
4.2.2缺点
（1）当数据量增大时，要求较大的内存支持I/O消耗也很大；如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
（2）当空间聚类的密度不均匀、聚类间距差相差很大时，聚类质量较差，因为这种情况下参数MinPts和Eps选取困难。
　3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。
4.2.3伪代码以及实现
输入：数据集D，给定点在邻域内成为核心对象的最小邻域点数：MinPts,邻域半径：Eps   
输出：簇集合
(1) 首先将数据集D中的所有对象标记为未处理状态
(2) for（数据集D中每个对象p） do
(3)    if （p已经归入某个簇或标记为噪声） then
(4)         continue;
(5)    else
(6)         检查对象p的Eps邻域 NEps(p) ；
(7)         if (NEps(p)包含的对象数小于MinPts) then
(8)                  标记对象p为边界点或噪声点；
(9)         else
(10)                 标记对象p为核心点，并建立新簇C, 并将p邻域内所有点加入C
(11)                 for (NEps(p)中所有尚未被处理的对象q)  do
(12)                       检查其Eps邻域NEps(q)，
若NEps(q)包含至少MinPts个对象，则将NEps(q)中未归入任何一个簇的对象加入C；
(13)                 end for
(14)        end if
(15)    end if
(16) end for
4.2.4时间复杂度 
（1）DBSCAN的基本时间复杂度是 O(N*找出Eps领域中的点所需要的时间), N是点的个数。最坏情况下时间复杂度是O(N2)
（2）在低维空间数据中,有一些数据结构如KD树，使得可以有效的检索特定点给定距离内的所有点，时间复杂度可以降低到O(NlogN)
空间复杂度：低维和高维数据中，其空间都是O(N)，对于每个点它只需要维持少量数据，即簇标号和每个点的标识(核心点或边界点或噪音点)
4.2.5 scikit-learn中的DBSCAN类
在scikit-learn中，DBSCAN算法类为sklearn.cluster.DBSCAN。
　DBSCAN类的重要参数也分为两类，一类是DBSCAN算法本身的参数，一类是最近邻度量的参数，下面我们对这些参数做一个总结。
1）eps： DBSCAN算法参数，即ϵ-邻域的距离阈值，和样本距离超过ϵ的样本点不在ϵ-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的ϵ-邻域，此时类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。
2）min_samples： DBSCAN算法参数，即样本点要成为核心对象所需要的ϵ-邻域的样本数阈值。默认值是5. 一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。
3）metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足需求。可以使用的距离度量参数有：
 
3）	algorithm：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。这三种方法在K近邻法(KNN)原理小结中都有讲述，如果不熟悉可以去复习下。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。
个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用"auto"建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。
5）leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。
6） p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。
以上就是DBSCAN类的主要参数介绍，其实需要调参的就是两个参数eps和min_samples，这两个值的组合对最终的聚类效果有很大的影响。
4.3高斯混合聚类
高斯混合模型采用概率模型来表示聚类原型。这种聚类方法可以得到每个样本点属于各个类的概率，而不是直接将样本点硬性划为某一类，因此也较为软聚类法。 
使用期望最大化的方法
第一步. 初始化K个高斯分布 本例中K=2 因为我们有两个原始数据集. ,我们取任意点,为每个高斯选择随机均值和随机方差 是方差 而不是标准差 方差是标准差的平方

第二步. 将数据软聚类成我们初始化的两个高斯分布 称为期望步骤或者E步骤  
数据集有N个点 每个点都有两个特征 ,每个点都具有相应的特征值,我们必须计算每个点对每个聚类的隶属度
 
第三步. 基于软聚类重新估计高斯 称为最大化步骤或者M步骤
我们有两个聚类,用这些来为高斯模型提出新的参数 所以第三步的目的 是利用第二步的结果作为输入,求出 针对聚类A以及B对应的新的均值和方差
 
 
第四步 我们评估对数似然函数检查收敛,,若收敛 则一切正常 返回结果 若不收敛 我们返回第二步,反复进行 直到收敛我们就会找到我们想要的两个高斯
 
 我们通过选择的参数 包括其混合系数 均值以及方差 来最大化这个值 直到算法收敛直到达到最大值 或每一步开始增加一小部分 就停止运算并且完成收敛了 然后选择那些模型作为高斯混合模型的组成部分 并在他们基础上聚类 这是进行高斯混合模型聚类期望最大化的最后一步
 
4.3.1优点以及缺点 
优点
1.可以为我们提供软聚类 软聚类是多个聚类的示例性隶属度 比如说 你正在进行文档分类 希望每个文档都是多个主题或者多个类别的一部分的情况下 GMM 聚类对于这些场景很有用
2.除此之外,在聚类外观方面也很具有灵活性 一个聚类可以包含另一个聚类
缺点: 1.GMM 聚类读初始化值很敏感
2.有可能收敛到局部最优, 3 但是收敛速度很慢
4.4特征提取
4.4.1 PCA
主成分分析（PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。
特征提取 是对数据进行转换,以生成新的有用特征
PCA(principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据压缩算法。在PCA中，数据从原来的坐标系转换到新的坐标系，由数据本身决定。转换坐标系时，以方差最大的方向作为坐标轴方向，因为数据的最大方差给出了数据的最重要的信息。第一个新坐标轴选择的是原始数据中方差最大的方法，第二个新坐标轴选择的是与第一个新坐标轴正交且方差次大的方向。重复该过程，重复次数为原始数据的特征维数。
那么，如何得到这些包含最大差异性的主成分方向呢？事实上，通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值及特征向量，选择特征值最大（也即包含方差最大）的N个特征所对应的特征向量组成的矩阵，我们就可以将数据矩阵转换到新的空间当中，实现数据特征的降维（N维）。
 
PCA步骤:
平均值
计算协方差矩阵
计算协方差矩阵的特征值和特征向量
将特征值排序
保留前N个最大的特征值对应的特征向量
将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩）
 
doPAC() 获取我的主成分分析 导入模块->创建->调整数据->作为对象返回
pca.explained_variance 方差比 其实是特征值的具体表现形式 通过打印出来从PCA对象中获取信息
认识到第一个主成分占据数据变动的90%或者91% 第二个主成分的占比约是9%或者10% 这些数字均是来自于 print pca.explained_variance 告诉我们的
接着研究第一个和第二个主成分 这个里面也只有两个主成分 因为传入的参数是 n_components=2,成分就是一份python列表 n_components 就是PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n
transformed_data=pca.transform(data) 得到降维之后的数据
4.4.2 scikit-learn PCA类
在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类就是sklearn.decomposition.PCA
　除了PCA类以外，最常用的PCA相关类还有KernelPCA类，它主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参。
　另外一个常用的PCA相关类是IncrementalPCA类，它主要是为了解决单机内存限制的。有时候我们的样本量可能是上百万+，维度可能也是上千，直接去拟合数据可能会让内存爆掉， 此时我们可以用IncrementalPCA类来解决这个问题。IncrementalPCA先将数据分成多个batch，然后对每个batch依次递增调用partial_fit函数，这样一步步的得到最终的样本最优降维。
　此外还有SparsePCA和MiniBatchSparsePCA。 区别主要是使用了L1的正则化，这样可以将很多非主要成分的影响度降为0，这样在PCA降维的时候仅仅需要对那些相对比较主要的成分进行PCA降维，避免了一些噪声之类的因素对我们PCA降维的影响。
SparsePCA和MiniBatchSparsePCA之间的区别则是MiniBatchSparsePCA通过使用一部分样本特征和给定的迭代次数来进行PCA降维，以解决在大样本时特征分解过慢的问题，当然，代价就是PCA降维的精确度可能会降低。使用SparsePCA和MiniBatchSparsePCA需要对L1正则化参数进行调参。
基于sklearn.decomposition.PCA来讲解如何使用scikit-learn进行PCA降维。
PCA类基本不需要调参，一般来说，只需要指定需要降维到的维度，或者希望降维后的主成分的方差和占原始维度所有特征方差和的比例阈值就可以了。
1）n_components：这个参数可以指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。
2）whiten ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。
3）svd_solver：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。
除了这些输入参数外，有两个PCA类的成员值得关注。第一个是explained_variance_，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是explained_variance_ratio_，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。
4.5特征缩放
             X---Xmin
     X' =   -----------------
             Xmax---Xmin
from sklearn.preprocessing import MinMaxScaler
import numpy
weights=numpy.array([[115.],[140.],[175.]])
scaler = MinMaxScaler()
#numpy 数组的每个元素都会成为不同的训练点,然后训练点中的每个元素都会成为特征
#拥有自己的权重特征数据输入之后,在该输入上使用缩放器
rescaled_weight=scaler.fit_transform(weights)

"""
#生成缩放 我要做的是在权重特征上调用 fit_transform 执行像查找最大值 最小值的操作,然后转换操作
#其实是针对所有数据集中的所有元素应用该公式  
但是会报错因为数组元素应该是浮点数 不应该是整数

"""
rescaled_weight
array([[0.        ],
       [0.41666667],
       [1.        ]])
SVM以及K均值聚类特征缩放会受到影
决策树以及线性回归则不会影响结果
4.6降维方法

1.	PCA  
PCA是一种数据降维的方法，但是只对符合高斯分布的样本点比较有效，那么对于其他分布的样本，有没有主元分解的方法
2.	.随机投影 
 比pca主成分分析更有效,通常应用于主成分分析无法直接计算的情况下
 
3.	独立成分分析(ICA)
主成分分析作用于最大化方差,而ICA假定这些特征是独立源的混合,并尝试分离这些混合在该数据集里面的独立源
 


五、强化学习(RL)
5.1基础
5.1.1.强化学习定义
强化学习是智能体（Agent）以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使智能体获得最大的奖赏
5.2 马尔卡夫决策过程(MDP)
马尔可夫决策过程(Markov Decision Process, MDP) 即系统下个状态不仅和当前的状态有关，也和当前采取的动作有关, 与更早之前的状态无关.一个马尔可夫决策过程由一个四元组构成M = (S, A, Psa, 𝑅), 
S状态  提供智能体的环境信息   S +,表示所有状态（包括终止状态）集合
A动作 是为了使智能体agent能够行走必须做出的决策   动作空间 A 是指智能体可以采取的动作集合。
𝑅 奖励 是告诉智能体选择了正确的动作 以反馈的一种形式 
𝑃s𝑎 表示的是在当前s ∈ S状态下，经过a ∈ A作用后，会转移到的其他状态的概率分布情况。
p(s'|s,a)。 表示在状态s下，执行行动a，状态变成s'的可能性。也可以写成 环境的状态转化模型
p(s′,r|s,a) . 表示在状态s下，执行行动a，状态变成s'，并获得奖赏r的可能性
MDP即假设转化到下一个状态s′的概率仅与上一个状态s有关，与之前的状态无关
 

G  回报 只有未来的奖励收到智能体的控制,我们将后续时间步的奖励之和,称之为回报, 智能体的目标是最大化预期回报

强化学习就是：追求最大回报G
追求最大回报G就是：找到最优的策略π*。
策略π*告诉在状态s，应该执行什么行动a。
最优策略可以由最优价值方法v*(s)或者q*(s,a)决定。

 
时间越远的奖励将乘以更小的值,称这种为折扣回报 

γ折扣率  折扣是指我们将更改目标,更关心近期的奖励,而不是遥远未来获得的奖励 乘以的值将如何选择呢 定义一个折扣率 用希腊字母γ表示 0到1之间的值 乘以γ ,γ^2....

 
A(s) 如果在某些状态下，只能采取部分动作，我们还可以使用 A(s) 表示在状态 s∈S 下可以采取的动作集合
π 策略 在某个状态下执行某种动作，这便是一种策略  
π(a|s) 策略π，在状态s下，选择的行动a的概率。即π(a|s)=P(At=a|St=s)
如果智能体想要跟踪策略,只需要指定该映射,将这种策略称之为决定性策略
随机性策略是指智能体能够随机的选择动作,将随机策略定义为以下映射 接收环境状态s和动作,并返回智能体在状态S时采取动作A的概率
r(s,a) 在状态s下，执行行动a的期望奖赏。

r(s,a)≐E[Rt+1|St=s,At=a]=  ∑ r  ∑   p(s′,r|s,a)
                   r∈R     s′∈S

vπ(s)状态值函数  在策略π和状态s时，采取行动后的价值（value）
状态值函数将始终对应特定的策略,如果我们更改策略,就会更改状态值函数
策略π的状态值函数是环境状态的函数 对于每个状态S 它都告诉我们 如果智能体从该状态S开始 然后在所有时间步根据该策略选择动作,预期的折扣回报是多少
vπ(s)=Eπ(Rt+1+γRt+2+γ2Rt+3+...|St=s)
    ------贝尔曼方程
γ是奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁
qπ(s,a) 动作值函数
智能体从状态s开始并选择动作a时获得的期望折扣回报，然后使用该策略选择所有未来时间步的动作。
 
 
状态值函数与动作值函数的对比:
 
5.3贝尔曼方程
在这个网格世界示例中，一旦智能体选择一个动作，它始终沿着所选方向移动（而一般 MDP 则不同，智能体并非始终能够完全控制下个状态将是什么）,可以确切地预测奖励（而一般 MDP 则不同，奖励是从概率分布中随机抽取的）。在此状态下,我们发现任何状态的值都可以计算成即时奖励和下个状态(折扣)值的和.
5.3.1 Vπ(S)状态值函数的贝尔曼预期方程
 
如果智能体的策略是确定性策略,智能体在状态S选择动作a -> π(s),贝尔曼方程为:
 
即将奖励和下个状态的折扣值之和与相应的概率相乘,并将所有概率相加得出预期值
如果智能体的策略是随机性策略,智能体在状态S选择a的概率是π(a|s) ,贝尔曼方程为:
 
 
总之贝尔曼预期方程  任何状态的值为 即时奖励加上下一个状态的值
5.3.2 qπ(s,a) 动作值函数贝尔曼预期方程
 
由于动作价值函数qπ(s,a) 和状态价值函数vπ(s) 的定义，我们很容易得到他们之间的转化关系:
 
状态价值函数是所有动作价值函数基于策略π的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。
 
5.3.3最优策略
 
5.3.4贝尔曼最优性方程
该方程可以证明:状态值以及动作值函数满足递归关系,可以将状态值或者状态动作对的值与所有后续状态(或者状态动作对)的值联系起来
 
5.4动态规划
在动态规划设置中，智能体完全了解 MDP（这比强化学习设置简单多了。在强化学习设置中，智能体一开始完全不了解环境如何确定状态和动作，必须完全通过互动来了解如何选择动作。）
5.4.1. 迭代策略评估
迭代方法 为了获得策略π 对应的状态值函数 vπ,我们只需求解 vπ的贝尔曼预期方程对应的方程组。 虽然可以通过分析方式求解方程组，但是我们将重点讲解迭代方法。
迭代策略评估是在动态规划设置中用到的算法，用于估算策略π 对应的状态值函数 vπ 。在此方法中，我们将向值函数估值中应用贝尔曼更新，直到估值的变化几乎不易觉察。
伪代码:
1.需要设置一个非常小的正数
2.值函数初始猜测全部为0
3.进行迭代循环,每一步应用卡尔曼更新规则,检查每个状态的值变化幅度
如果所有状态的最大变化,小于所设置的小正数,则停止该算法,表示值函数的估值已经很不错


 
	主要代码实现
 
你的算法应该有四个输入参数：
•	env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。
•	policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。
•	gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。
•	theta：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：1e-8）。
该算法会返回以下输出结果：
•	V：这是一个一维numpy数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 在输入策略下的估算值。
import numpy as np

def policy_evaluation(env, policy, gamma=1, theta=1e-8):
    V = np.zeros(env.nS)# 初始化状态值为0
    while True:
        delta = 0  # 变化值为0
        for s in range(env.nS):
            Vs = 0
            for a, action_prob in enumerate(policy[s]):
                for prob, next_state, reward, done in env.P[s][a]:
                    Vs += action_prob * prob * (reward + gamma * V[next_state])
            delta = max(delta, np.abs(V[s]-Vs))
            V[s] = Vs
        if delta < theta:
            break
return V
注意：只要对于每个状态π(s) 是有限的，策略评估就保证会收敛于策略π 对应的状态值函数。对于有限的马尔可夫决策流程 (MDP)，只要满足以下条件之一，就保证会收敛： γ<1， 或如果智能体以任何状态 s∈S 开始，并且遵守 π，就保证会最终达到终止状态
动作值的估值：
在动态规划设置中，可以使用以下方程从状态值函数  vπ快速获得动作值函数 qπ

 
 
代码:

该函数的输入是状态值函数估值以及一些状态 s∈S。它会返回输入状态 s∈S 对应的动作值函数中的行。即你的函数应同时接受输入 vπ和 s，并针对所有 a∈A(s) 返回 qπ(s,a)。
你的算法应该有四个输入参数：
•	env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。
•	V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。
•	s：这是环境中的状态对应的整数。它应该是在 0 到 (env.nS)-1（含）之间的值。
•	gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。
该算法会返回以下输出结果：
•	q：这是一个一维 numpy 数组，其中 q.shape[0] 等于动作数量 (env.nA)。q[a] 包含状态 s 和动作 a 的（估算）值。

def q_from_v(env, V, s, gamma=1):
    q = np.zeros(env.nA)
    for a in range(env.nA):
        for prob, next_state, reward, done in env.P[s][a]:
            q[a] += prob * (reward + gamma * V[next_state])
    return q

Q = np.zeros([env.nS, env.nA])
for s in range(env.nS):
    Q[s] = q_from_v(env, V, s)
print("Action-Value Function:")
print(Q)

5.4.2策略改进
策略改进 算法使用策略的值函数 提出一个至少和当前策略一样好的新策略
策略评估获得一个策略并生成一个值函数 ,然后使用该值函数和策略完善方法获得一个潜在完善的新策略,然后带入该新策略,再次进行策略评估,然后进行策略完善,不断重复下去直到我们收敛于最优策略 这是阶段性任务
 
 

策略改进代码实现。
你的算法应该有三个输入参数：
•	env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。
•	V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。
•	gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。
该算法会返回以下输出结果：
•	policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。
请完成以下代码单元格中的函数。建议你使用你在上文实现的 q_from_v 函数。
def policy_improvement(env, V, gamma=1):
    policy = np.zeros([env.nS, env.nA]) / env.nA
    for s in range(env.nS):
        q = q_from_v(env, V, s, gamma)
        
        # OPTION 1: construct a deterministic policy 
        # policy[s][np.argmax(q)] = 1
        
        # OPTION 2: construct a stochastic policy that puts equal probability on maximizing actions
        best_a = np.argwhere(q==np.max(q)).flatten()
        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)
        
return policy

5.4.3策略迭代
 
该算法对最优策略进行初始猜测,从对等随机策略开始,对于每个状态,选择每个动作的概率是一样的 然后通过策略评估获得相应的值函数,通过策略完善获得对等或者更好 的策略 然后通过策略评估和策略完善 重复循环这一过程 知道没有变化

 
你的算法应该有三个输入参数：
•	env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。
•	gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。
•	theta：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：1e-8）。
该算法会返回以下输出结果：
•	policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。
•	V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。
请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 policy_evaluation 和 policy_improvement 函数。
import copy

def policy_iteration(env, gamma=1, theta=1e-8):
    policy = np.ones([env.nS, env.nA]) / env.nA
    while True:
        V = policy_evaluation(env, policy, gamma, theta)
        new_policy = policy_improvement(env, V)
        
        # OPTION 1: stop if the policy is unchanged after an improvement step
        if (new_policy == policy).all():
            break;
        
        # OPTION 2: stop if the value function estimates for successive policies has converged
        # if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:
        #    break;
        
        policy = copy.copy(new_policy)
    return policy, V
5.4.4截断策略迭代
（迭代性）策略评估会根据需要应用很多次贝尔曼更新步骤，以实现收敛，而截断策略迭代仅对整个状态空间执行固定次数的评估。


 
 
 
5.4.5值迭代
 
值迭代。在此算法中，对状态空间的每次遍历都会进行策略评估和策略改进。值迭代肯定会找到任何有限 MDP 的最优策略 π∗
通过不断评估和改进找到最优策略（在遍历整个状态空间一次后停止评估步骤）
 
5.5蒙特卡洛方法(MC)
5.5.1. MC 预测（状态值)
蒙特卡洛法:如果 求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解
如果状态在某个阶段中经历了多次:
首次经历MC方法:将Vπ(s)估算仅仅在s首次经历之后的回报.在每个阶段 只考虑该状态的首次经历 然后对这些回报去均值
所有经历MC预测 是对所有阶段中,该状态的所有经历之后的回报取均值
 
5.5.2. MC预测 动作值
查看每个潜在状态动作对的经历,然后计算每个状态动作对之后的回报,并像之前一样取平均值
首次经历MC方法:将qπ(s,a)估算仅仅在s,a首次经历之后的回报.在每个阶段 只考虑该状态的首次经历 然后对这些回报去均值
所有经历MC预测 是对所有阶段中, 将qπ(s,a)估算为s,a所有经历之后的回报取均值

 
5.5.3 MC 控制 增量均值
动态规划设置流程中首先了解的控制算法是策略迭代 即执行一系列的评估和完善步骤,使评估步骤不断接近收敛效果
阶段策略迭代有所不同 不在强制评估步骤达到收敛效果 只是对状态空间执行一定数量的循环过程
值迭代 是指仅仅完成一次评估步骤然后进入完善步骤,将这种一般流程称为广义策略迭代
假设该状态动作对经历了一定的次数,将相应的回报表示为X1,X2…Xn,然后通过对这些值取均值,计算值函数逼近结果,表示为Un ,n表示尽力该状态动作对n次,不再是所有阶段结束之后计算该平均值,而是在每次经历之后都有迭代更新估值
   
第k项均值 等于 对K减去第一个均值和第k项回报
先将均值初始化为0 μ =0
还需要记录平均值中已经包含的回报的数量 开始初始化为0 k=0
然后进入while循环
每次使K加1 然后使用最近的回报Xk更新均值

 
5.5.4 MC 控制：策略评估
智能体先从环境中获得一个阶段样例 然后对于每个时间步,都查看相应的状态动作对 如果是首次经历则计算相应的回报,然后根据不断取平均值的算法.更新动作值的相应估值
 
5.5.5 MC 控制：策略改进
直接选择值最高的动作---贪婪算法
但是更好的是采取随机性策略,以非常高的概率选择贪婪动作,但是以很小的非0概率 选择某个非贪婪动作. 这种情况下 设置一个非常小的正数e, 此值越大 就越可能选择非贪婪动作, 称为epsilon贪婪算法 ,e必须是0到1之间的数字,智能体以1-e的概率,选择贪婪动作,并以e的概率在一组潜在(贪婪和非贪婪)动作中均匀随机选择任何一个动作
为了构建一个相对于当前动作值函数估值 Q 为 ϵ 贪婪策略的策略 π，我们只需设置如下:
 
要将e设置成很小的一个数字就可以构建接近于贪婪策略的策略,但是同时又不会阻止智能体去探索其他的可能性
 
为了使MC控制收敛于最优策略,必须满足于有限状态下的无线探索贪婪算法(GLIE)
对于所有时间步 i，ϵ i>0¶
当时间步 i 接近无穷大时，ϵi减小到 0（即lim i→∞ϵ i =0）。
ϵ=1 会生成一个等同于等概率随机策略的ϵ 贪婪策略,去探索环境,随机策略
ϵ=0 会生成贪婪策略（或者非常倾向于利用已有的经验而不是探索环境的策略）。
 
5.5.6MC 控制：常量 α 
有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数N(St),这时我们可以用一个系数α来代替，常量步长α必须满足0<=α<=1 常量越大学习速度越快,但是值过大可能导致MC控制无法收敛于π
可以确保稍后获得的回报比之前获得的回报更加受重视,智能体将更信任最新的回报¶
 
5.6动态规划和MC对比分析
5.6.1动态规划
1.预测问题  即给定强化学习的6个要素,状态集S, 动作集A, 模型状态转化概率矩阵P, 即时奖励R，衰减因子γ,  给定策略π，求解该状态的价值函数v(π)
2.控制问题  给定强化学习的5个要素：状态集S, 动作集A, 模型状态转化概率矩阵P, 即时奖励R，衰减因子γ, 求解最优的状态价值函数v*和最优策略π*
模型状态转化概率矩阵P始终是已知的,，即MDP已知，称为基于模型的强化学习问题。
5.6.2 MC
不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵P，这时如果仍然需要我们求解强化学习问题，那么这就是不基于模型的强化学习问题了。
它的两个问题一般的定义是：
1.预测问题  即给定强化学习的5个要素：状态集S, 动作集A, 即时奖励R，衰减因子γ,  给定策略π， 求解该策略的状态价值函数v(π)
2.控制问题，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集S, 动作集A, 即时奖励R，衰减因子γ, 探索率ϵ, 求解最优的动作价值函数q*和最优策略π*
蒙特卡罗法就是上述不基于模型的强化学习问题。
MC 一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。
预测问题即策略评估
一个给定策略ππ的完整有T个状态的状态序列如下：
S1,A1,R2,S2,A2,...St,At,Rt+1,...RT,ST
MDP中对于价值函数vπ(s)的定义:
vπ(s)=Eπ(Gt|St=s)=Eπ(Rt+1+γRt+2+γ2Rt+3+...|St=s)
每个状态的价值函数等于所有该状态收获的期望,同时这个收获是通过后续的奖励与对应的衰减乘积求和得到
蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：
Gt=Rt+1+γRt+2+γ2Rt+3+...γT−t−1RT
vπ(s)≈average(Gt),s.t.St=s

同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？有两种解决方法。第一种是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：首次访问(first visit) 和每次访问(every visit) 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。
迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。
 
这样上面的状态价值公式就可以改写成：
 
尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数N(St),这时我们可以用一个系数α来代替，即：
V(St)=V(St)+α(Gt−V(St))
对于动作价值函数Q(St,At)也是类似的，比如对上面最后一个式子，动作价值函数版本为：
Q(St,At)=Q(St,At)+α(Gt−Q(St,At))
控制问题:
蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。回忆下动态规划价值迭代的的思路， 每轮迭代先做策略评估，计算出价值vk(s)，然后基于据一定的方法（比如贪婪法）更新当前策略π。最后得到最优价值函数v∗和最优策略π∗。
蒙特卡罗法不同之处体现在三点：一是预测问题策略评估的方法不同。第二是蒙特卡罗法一般是优化最优动作价值函数q∗，而不是状态价值函数v∗。三是动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用ϵ−贪婪法更新策略
ϵ−贪婪法通过设置一个较小的ϵ值，使用1−ϵ的概率贪婪地选择目前认为是最大行为价值的行为，而用ϵ 的概率随机的从所有m 个可选行为中选择行为
 
在实际求解控制问题时，为了使算法可以收敛，一般ϵ会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。
算法流程:
用的是MC所有经历 即个状态序列中每次出现的相同状态，都会计算对应的收获值。
输入：状态集S, 动作集A, 即时奖励R，衰减因子γ, 探索率ϵ
输出：最优的动作价值函数q*和最优策略π*
1. 初始化所有的动作价值Q(s,a)=0， 状态次数N(s,a)=0，采样次数k=0，随机初始化一个策略π
2.  k=k+1, 基于策略π进行第k次蒙特卡罗采样，得到一个完整的状态序列:
S1,A1,R2,S2,A2,...St,At,Rt+1,...RT,ST
3. 对于该状态序列里出现的每一状态行为对(St,At)，计算其收获Gt, 更新其计数N(s,a)和行为价值函数Q(s,t)
 
 4. 基于新计算出的动作价值，更新当前的ϵ−贪婪策略：
 
5. 如果所有的Q(s,a)收敛，则对应的所有Q(s,a)即为最优的动作价值函数q∗。对应的策略π(a|s)即为最优策略π∗。否则转到第二步。
蒙特卡罗法是我们第二个讲到的求解强化问题的方法，也是第一个不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点，这就是它每次采样都需要一个完整的状态序列。如果我们没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了， 也就是说，我们还需要寻找其他的更灵活的不基于模型的强化问题求解方法。时序差分法和蒙特卡罗法类似，都是不基于模型的强化学习问题求解方法。

5.6 时间差分(TD)
5.6.1. 时间差分(TD)
虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化学习问题的方法：时序差分(Temporal-Difference, TD)。时序差分法和蒙特卡罗法类似，都是不基于模型的强化学习问题求解方法

预测问题：即给定强化学习的5个要素：状态集S, 动作集A, 即时奖励R，衰减因子γ,  给定策略π， 求解该策略的状态价值函数v(π)

控制问题：也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集S, 动作集A, 即时奖励R，衰减因子γ, 探索率ϵ, 求解最优的动作价值函数q*和最优策略π*
蒙特卡罗法中计算状态收获的方法是：
Gt=Rt+1+γRt+2+γ2Rt+3+...γT−t−1RT
vπ(s)≈average(Gt),s.t.St=s
而对于时序差分法来说，我们没有完整的状态序列，只有部分的状态序列，那么如何可以近似求出某个状态的收获呢
MDP中贝尔曼方程:
vπ(s)=Eπ(Rt+1+γvπ(St+1)|St=s)
这启发我们可以用Rt+1+γv(St+1)来近似的代替收获Gt, Rt+1+γV(St+1)称为TD目标值。
Rt+1+γV(St+1)−V(St)称为TD误差
 5.6.2.时序差分TD的预测
状态值:

时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点。一是收获Gt的表达式不同，时序差分G(t)的表达式为：
G(t)=Rt+1+γV(St+1)
二是迭代的式子系数稍有不同，回顾蒙特卡罗法的迭代式子是：
  
由于在时序差分我们没有完整的序列，也就没有对应的次数N(St),一般就用一个[0,1]的系数α代替。这样时序差分的价值函数迭代式子是：
V(St)=V(St)+α(Gt−V(St))
Q(St,At)=Q(St,At)+α(Gt−Q(St,At))
其中G(t)=Rt+1+γV(St+1)
连续性任务:
1.现将每个状态的值初始化为0
2.在每个时间步智能体与环境互动,选择由策略决定的动作,从环境中获得奖励和下个状态后.就立即更新上个状态的值函数
 
阶段性任务:
只需要检查在每个时间步 最近的状态是否为最终状态 如果是 则最后一次运行更新步骤以便更新上个状态,然后开启新的阶段
 
实现:
你的 TD 预测算法将包括 5 个参数：
•	env：这是 OpenAI Gym 环境的实例。
•	num_episodes：这是通过智能体-环境互动生成的阶段次数。
•	policy：这是一个一维 numpy 数组，其中 policy.shape 等于状态数量 (env.nS)。policy[s] 返回智能体在状态 s 时选择的动作。
•	alpha：这是更新步骤的步长参数。
•	gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。
该算法会返回以下输出结果：
•	V：这是一个字典，其中 V[s] 是状态 s 的估算值。
from collections import defaultdict, deque
import sys
def td_prediction(env, num_episodes, policy, alpha, gamma=1.0):
    # initialize empty dictionaries of floats
    V = defaultdict(float)
    # loop over episodes
    for i_episode in range(1, num_episodes+1):
        # monitor progress
        if i_episode % 100 == 0:
            print("\rEpisode {}/{}".format(i_episode, num_episodes), end="")
            sys.stdout.flush()
        # begin an episode, observe S
        state = env.reset()
        while True:
            # choose action A
            action = policy[state]
            # take action A, observe R, S'
            next_state, reward, done, info = env.step(action)
            # perform updates
            V[state] = V[state] + (alpha * (reward + (gamma * V[next_state]) - V[state]))            
            # S <- S'
            state = next_state
            # end episode if reached terminal state
            if done:
                break   
    return V
import check_test
# evaluate the policy and reshape the state-value function
V_pred = td_prediction(env, 5000, policy, .01)
# please do not change the code below this line
V_pred_plot = np.reshape([V_pred[key] if key in V_pred else 0 for key in np.arange(48)], (4,12)) 
check_test.run_check('td_prediction_check', V_pred_plot)
plot_values(V_pred_plot)
5.6.2 TD 预测：动作值
智能体将在每次选择动作后都更新值 而不是在接收到每个状态后更新值 唯一区别

 
 
假设我们的强化学习问题有A,B两个状态，模型未知，不涉及策略和行为。只涉及状态转化和即时奖励。一共有8个完整的状态序列如下：
　　　　① A,0,B,0 ②B,1 ③B,1 ④ B,1 ⑤ B,1 ⑥B,1 ⑦B,1 ⑧B,0
　　　　只有第一个状态序列是有状态转移的，其余7个只有一个状态。设置衰减因子γ=1。
　　　　首先我们按蒙特卡罗法来求解预测问题。由于只有第一个序列中包含状态A，因此A的价值仅能通过第一个序列来计算，也就等同于计算该序列中状态A的收获：
V(A)=G(A)=RA+γRB=0
　　　　对于B，则需要对其在8个序列中的收获值来平均，其结果是6/8。
　　　　再来看看时序差分法求解的过程。其收获是在计算状态序列中某状态价值时是应用其后续状态的预估价值来计算的，对于B来说，它总是终止状态，没有后续状态，因此它的价值直接用其在8个序列中的收获值来平均，其结果是6/8。
　　　　对于A，只在第一个序列出现，它的价值为：
V(A)=RA+γV(B)=6/8
5.6.3 MC与TD预测问题的区别
一是时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到预测必须等到阶段结束时才能更新值函数估值,才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。
二是时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。
三是虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。
可以看出时序差分法的优势比较大，因此现在主流的强化学习求解方法都是基于时序差分的
5.6.4  TD 控制
控制问题可以表示为：给定强化学习的5个要素：状态集S, 动作集A, 即时奖励R，衰减因子γ, 探索率ϵ, 求解最优的动作价值函数q*和最优策略π*。

MC在线控制的方法，我们使用的是ϵ−贪婪法来做价值迭代,对于时序差分，我们也可以用ϵ−贪婪法来价值迭代，和MC在线控制的区别主要只是在于收获的计算方式不同。时序差分的在线控制(on-policy)算法最常见的是SARSA算法

离线控制和在线控制的区别主要在于在线控制一般只有一个策略(最常见的是ϵ−贪婪法) 即一直使用一个策略来更新价值函数和选择新的动作。而离线控制一般有两个策略，其中一个策略(最常见的是ϵ−贪婪法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。时序差分的离线控制算法最常见的是Q-Learning算法

TD求解不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。
5.6.4.1 SARSA算法  实际上是S,A,R,S,A
SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是ϵ−贪婪法

对于ϵ−贪婪法有详细讲解，即通过设置一个较小的ϵ值，使用1−ϵ的概率贪婪地选择目前认为是最大行为价值的行为，而用ϵ的概率随机的从所有m个可选行为中选择行为。用公式可以表示为：
 
 
在迭代的时候，首先基于ϵ−贪婪法在当前状态S选择一个动作A，这样系统会转到一个新的状态S′, 同时给我们一个即时奖励R, 在新的状态S′，基于ϵ−ϵ−贪婪法在状态S’选择一个动作A′，但是注意这时候我们并不执行这个动作A′，只是用来更新的我们的价值函数，价值函数的更新公式是：
Q(S,A)=Q(S,A)+α(R+γQ(S′,A′)−Q(S,A))
其中，γ是衰减因子，α是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获Gt的表达式不同，对于时序差分，收获Gt的表达式是Q(S,A)=R+γQ(S′,A′)
5.6.4.2 SARSA 算法流程
算法输入：迭代轮数T，状态集S, 动作集A, 步长α，衰减因子γ, 探索率ϵ,
输出：所有的状态和动作对应的价值Q
1. 随机初始化所有的状态和动作对应的价值Q. 对于终止状态其Q值初始化为0.
2. for i from 1 to T，进行迭代。
　　a) 初始化S为当前状态序列的第一个状态。设置A为ϵ−贪婪法在当前状态S选择的动作。
　　b) 在状态S执行当前动作A,得到新状态S′和奖励R
　　c) 用ϵ−贪婪法在状态S′选择新的动作A′
　　d) 更新价值函数Q(S,A):
Q(S,A)=Q(S,A)+α(R+γQ(S′,A′)−Q(S,A)) 
e) S=S′,A=A′ 
f) 如果S′是终止状态，当前轮迭代完毕，否则转到步骤b)
这里有一个要注意的是，步长α一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数Q可以收敛。当Q收敛时，我们的策略ϵ−贪婪法也就收敛了。
 
但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，Q(S,A)的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。
5.6.4.3 TD 控制：Sarsamax  ----- Q-Learning
时序差分离线控制算法
Q-Learning算法的拓补图入下图所示
 

首先我们基于状态S，用ϵ−贪婪法选择到动作A, 然后执行动作A，得到奖励R，并进入状态S′，此时，如果是SARSA，会继续基于状态S′，用ϵ−贪婪法选择A′,然后来更新价值函数。Q(S,A)=Q(S,A)+α(R+γQ(S′,A′)−Q(S,A)) 

但是对于Q-Learning，它基于状态S′，没有使用ϵ−贪婪法选择A′，而是使用贪婪法选择A′，也就是说，选择使Q(S′,a)最大的a作为A′来更新价值函数。用数学公式表示就是：
 
对应到上图中就是在图下方的三个黑圆圈动作中选择一个使Q(S′,a)最大的动作作为A′。
此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态S′，用ϵ−贪婪法重新选择得到,这一点也和SARSA稍有不同。
对于SARSA，价值函数更新使用的A′会作为下一阶段开始时候的执行动作。
5.6.4.4 Q-Learning算法流程
算法输入：迭代轮数T，状态集S, 动作集A, 步长α，衰减因子γ, 探索率ϵ,
输出：所有的状态和动作对应的价值Q
1. 随机初始化所有的状态和动作对应的价值Q. 对于终止状态其Q值初始化为0.
2. for i from 1 to T，进行迭代。
　a) 初始化S为当前状态序列的第一个状态。
　b) 用ϵ−贪婪法在当前状态S选择出动作A
　c) 在状态S执行当前动作A,得到新状态S′和奖励R
　d)  更新价值函数Q(S,A):
 　e) S=S′
　f) 如果S′是终止状态，当前轮迭代完毕，否则转到步骤b)
 
5.6.4.5预期 Sarsa
与Q学习很类似 区别就是动作值的更新
Q学习是对所有的可能的下个状态动作都取最大动作 通过带入最大化下个状态对应的动作估值选择在此处采取的值
预期sara不同 采取的是下个状态动作对的预期值,会考虑智能体从下个状态选择每个可能的动作的概率

 
 
5.6.4.6分析性能
在以下情况下，我们讨论过的所有 TD 控制算法（Sarsa、Sarsamax、预期 Sarsa）都会收敛于最优动作值函数 q∗,并生成最优策略 π∗：
(1)ϵ 的值根据 GLIE 条件逐渐降低，以及
(2) 步长参数 α 足够小。
5.6.5这些算法之间的区别总结
@ Sarsa 和预期 Sarsa 都是异同策略 TD 控制算法。在这种情况下，我们会根据要评估和改进的相同（ϵ 贪婪策略）策略选择动作。
@ Sarsamax 是离线策略方法，我们会评估和改进（\epsilonϵ 贪婪）策略，并根据另一个策略选择动作。
@ 既定策略 TD 控制方法（例如预期 Sarsa 和 Sarsa）的在线效果比新策略 TD 控制方法（例如 Sarsamax）的要好。
@预期 Sarsa 通常效果比 Sarsa 的要好。
Q 学习的在线效果更差（智能体在每个阶段平均收集的奖励更少），但是能够学习最优策略，以及 Sarsa 可以获得更好的在线效果，但是学到的是次最优“安全”策略。
 

